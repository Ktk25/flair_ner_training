{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bda89a1-1fa6-485c-87ce-bd52f5b1783d",
   "metadata": {},
   "source": [
    "### Loading the TSV DATA using Flair Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39fcd0cb-b8df-4fd9-a093-e493eca2db75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-26 18:33:35,471 Reading data from data\n",
      "2022-02-26 18:33:35,472 Train: data\\conllpp_train.txt\n",
      "2022-02-26 18:33:35,472 Dev: data\\conllpp_dev.txt\n",
      "2022-02-26 18:33:35,472 Test: data\\conllpp_test.txt\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Corpus\n",
    "from flair.datasets import ColumnCorpus\n",
    "\n",
    "columns = {0:'text', 1:'pos', 2:'chunk', 3:'ner'}\n",
    "data_file = 'data/'\n",
    "\n",
    "# initializing the corpus\n",
    "corpus: Corpus = ColumnCorpus(data_file, columns,\n",
    "                              train_file = 'conllpp_train.txt',\n",
    "                              test_file = 'conllpp_test.txt',\n",
    "                              dev_file = 'conllpp_dev.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ced4053-97d5-421f-9652-03ddf98b7880",
   "metadata": {},
   "source": [
    "#### Taking  look at one sample sentence with given NER Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ef0f75d-ba3b-47bf-8970-6ebf72b01dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"State media quoted China <B-LOC> 's top negotiator with Taipei <B-LOC> , Tang <B-PER> Shubei <I-PER> , as telling a visiting group from Taiwan <B-LOC> on Wednesday that it was time for the rivals to hold political talks .\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.train[35].to_tagged_string('ner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd073a4-416f-4b7b-97de-c813e3231a70",
   "metadata": {},
   "source": [
    "#### Making word:NER dictionary for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9000487f-0ace-4837-ac55-f2cdc9f1a2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\karti\\AppData\\Local\\Temp/ipykernel_12552/1387472010.py:4: DeprecationWarning: Call to deprecated method make_tag_dictionary. (Use 'make_label_dictionary' instead.) -- Deprecated since version 0.8.\n",
      "  tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n"
     ]
    }
   ],
   "source": [
    "# tag to predict\n",
    "tag_type = 'ner'\n",
    "# make tag dictionary from the corpus\n",
    "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe30936-ba86-4084-ade2-b8cadc3cdf82",
   "metadata": {},
   "source": [
    "#### Defining Embeddings to convert INPUT -> Embedddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88a3f7b0-876b-4895-b1a5-ca10faf01de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import WordEmbeddings, StackedEmbeddings, TokenEmbeddings\n",
    "from typing import List\n",
    "embedding_types : List[TokenEmbeddings] = [\n",
    "        WordEmbeddings('glove'),\n",
    "        ## other embeddings\n",
    "        ]\n",
    "embeddings : StackedEmbeddings = StackedEmbeddings(\n",
    "                                 embeddings=embedding_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564e0832-c814-4679-9837-af49d963f34f",
   "metadata": {},
   "source": [
    "#### Denfining the Type of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5f8e3be-420b-4987-b9cb-35bcbbd33bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceTagger(\n",
      "  (embeddings): StackedEmbeddings(\n",
      "    (list_embedding_0): WordEmbeddings(\n",
      "      'glove'\n",
      "      (embedding): Embedding(400001, 100)\n",
      "    )\n",
      "  )\n",
      "  (word_dropout): WordDropout(p=0.05)\n",
      "  (locked_dropout): LockedDropout(p=0.5)\n",
      "  (embedding2nn): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (rnn): LSTM(100, 256, batch_first=True, bidirectional=True)\n",
      "  (linear): Linear(in_features=512, out_features=11, bias=True)\n",
      "  (beta): 1.0\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from flair.models import SequenceTagger\n",
    "tagger : SequenceTagger = SequenceTagger(hidden_size=256,\n",
    "                                       embeddings=embeddings,\n",
    "                                       tag_dictionary=tag_dictionary,\n",
    "                                       tag_type=tag_type,\n",
    "                                       use_crf=True)\n",
    "print(tagger)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f9a699-cf8f-4116-b5b7-d04d23261429",
   "metadata": {},
   "source": [
    "### Defining Training Parameters\n",
    "\n",
    "**Parameters:**  \n",
    "learning_rate: \"0.1\"  \n",
    "mini_batch_size: \"32\"  \n",
    "patience: \"3\"  \n",
    "anneal_factor: \"0.5\"  \n",
    "max_epochs: \"150\"  \n",
    "shuffle: \"True\"  \n",
    "train_with_dev: \"False\"  \n",
    "batch_growth_annealing: \"False\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c3b7608-2faa-4008-a4d4-c50f6092d880",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-26 18:43:53,563 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 18:43:53,564 Model: \"SequenceTagger(\n",
      "  (embeddings): StackedEmbeddings(\n",
      "    (list_embedding_0): WordEmbeddings(\n",
      "      'glove'\n",
      "      (embedding): Embedding(400001, 100)\n",
      "    )\n",
      "  )\n",
      "  (word_dropout): WordDropout(p=0.05)\n",
      "  (locked_dropout): LockedDropout(p=0.5)\n",
      "  (embedding2nn): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (rnn): LSTM(100, 256, batch_first=True, bidirectional=True)\n",
      "  (linear): Linear(in_features=512, out_features=11, bias=True)\n",
      "  (beta): 1.0\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n",
      "2022-02-26 18:43:53,564 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 18:43:53,565 Corpus: \"Corpus: 14987 train + 3466 dev + 3684 test sentences\"\n",
      "2022-02-26 18:43:53,566 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 18:43:53,566 Parameters:\n",
      "2022-02-26 18:43:53,567  - learning_rate: \"0.1\"\n",
      "2022-02-26 18:43:53,567  - mini_batch_size: \"32\"\n",
      "2022-02-26 18:43:53,568  - patience: \"3\"\n",
      "2022-02-26 18:43:53,568  - anneal_factor: \"0.5\"\n",
      "2022-02-26 18:43:53,568  - max_epochs: \"150\"\n",
      "2022-02-26 18:43:53,569  - shuffle: \"True\"\n",
      "2022-02-26 18:43:53,569  - train_with_dev: \"False\"\n",
      "2022-02-26 18:43:53,570  - batch_growth_annealing: \"False\"\n",
      "2022-02-26 18:43:53,570 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 18:43:53,570 Model training base path: \"resources\\taggers\\example-ner\"\n",
      "2022-02-26 18:43:53,571 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 18:43:53,571 Device: cuda:0\n",
      "2022-02-26 18:43:53,571 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 18:43:53,572 Embeddings storage mode: cpu\n",
      "2022-02-26 18:43:53,574 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 18:44:00,557 epoch 1 - iter 46/469 - loss 0.77736460 - samples/sec: 211.01 - lr: 0.100000\n",
      "2022-02-26 18:44:05,892 epoch 1 - iter 92/469 - loss 0.63639102 - samples/sec: 276.08 - lr: 0.100000\n",
      "2022-02-26 18:44:10,790 epoch 1 - iter 138/469 - loss 0.57304353 - samples/sec: 300.84 - lr: 0.100000\n",
      "2022-02-26 18:44:15,367 epoch 1 - iter 184/469 - loss 0.52729020 - samples/sec: 321.84 - lr: 0.100000\n",
      "2022-02-26 18:44:20,082 epoch 1 - iter 230/469 - loss 0.48965363 - samples/sec: 312.30 - lr: 0.100000\n",
      "2022-02-26 18:44:25,732 epoch 1 - iter 276/469 - loss 0.45804052 - samples/sec: 260.64 - lr: 0.100000\n",
      "2022-02-26 18:44:31,433 epoch 1 - iter 322/469 - loss 0.43630064 - samples/sec: 258.62 - lr: 0.100000\n",
      "2022-02-26 18:44:36,776 epoch 1 - iter 368/469 - loss 0.42341076 - samples/sec: 275.77 - lr: 0.100000\n",
      "2022-02-26 18:44:42,934 epoch 1 - iter 414/469 - loss 0.40839538 - samples/sec: 239.29 - lr: 0.100000\n",
      "2022-02-26 18:44:49,156 epoch 1 - iter 460/469 - loss 0.39216807 - samples/sec: 236.76 - lr: 0.100000\n",
      "2022-02-26 18:44:50,122 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 18:44:50,123 EPOCH 1 done: loss 0.3905 - lr 0.1000000\n",
      "2022-02-26 18:44:59,812 DEV : loss 0.20086319744586945 - f1-score (micro avg)  0.691\n",
      "2022-02-26 18:44:59,903 BAD EPOCHS (no improvement): 0\n",
      "2022-02-26 18:44:59,904 saving best model\n",
      "2022-02-26 18:45:00,496 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 18:45:05,601 epoch 2 - iter 46/469 - loss 0.27104635 - samples/sec: 288.69 - lr: 0.100000\n",
      "2022-02-26 18:45:10,734 epoch 2 - iter 92/469 - loss 0.26401798 - samples/sec: 286.97 - lr: 0.100000\n",
      "2022-02-26 18:45:15,819 epoch 2 - iter 138/469 - loss 0.25707867 - samples/sec: 289.60 - lr: 0.100000\n",
      "2022-02-26 18:45:20,799 epoch 2 - iter 184/469 - loss 0.25588031 - samples/sec: 295.61 - lr: 0.100000\n",
      "2022-02-26 18:45:25,837 epoch 2 - iter 230/469 - loss 0.25589093 - samples/sec: 292.40 - lr: 0.100000\n",
      "2022-02-26 18:45:30,955 epoch 2 - iter 276/469 - loss 0.25122732 - samples/sec: 287.82 - lr: 0.100000\n",
      "2022-02-26 18:45:35,919 epoch 2 - iter 322/469 - loss 0.24922853 - samples/sec: 296.60 - lr: 0.100000\n",
      "2022-02-26 18:45:41,184 epoch 2 - iter 368/469 - loss 0.24522170 - samples/sec: 279.72 - lr: 0.100000\n",
      "2022-02-26 18:45:46,428 epoch 2 - iter 414/469 - loss 0.24185981 - samples/sec: 281.00 - lr: 0.100000\n",
      "2022-02-26 18:45:51,682 epoch 2 - iter 460/469 - loss 0.23953136 - samples/sec: 280.51 - lr: 0.100000\n",
      "2022-02-26 18:45:52,651 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 18:45:52,652 EPOCH 2 done: loss 0.2389 - lr 0.1000000\n",
      "2022-02-26 18:46:00,057 DEV : loss 0.15043145418167114 - f1-score (micro avg)  0.7693\n",
      "2022-02-26 18:46:00,145 BAD EPOCHS (no improvement): 0\n",
      "2022-02-26 18:46:00,146 saving best model\n",
      "2022-02-26 18:46:00,702 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 18:46:05,951 epoch 3 - iter 46/469 - loss 0.20652022 - samples/sec: 280.79 - lr: 0.100000\n",
      "2022-02-26 18:46:11,120 epoch 3 - iter 92/469 - loss 0.20591369 - samples/sec: 284.92 - lr: 0.100000\n",
      "2022-02-26 18:46:16,208 epoch 3 - iter 138/469 - loss 0.20646098 - samples/sec: 289.40 - lr: 0.100000\n",
      "2022-02-26 18:46:21,398 epoch 3 - iter 184/469 - loss 0.20736588 - samples/sec: 283.77 - lr: 0.100000\n",
      "2022-02-26 18:46:26,420 epoch 3 - iter 230/469 - loss 0.20693952 - samples/sec: 293.29 - lr: 0.100000\n",
      "2022-02-26 18:46:31,550 epoch 3 - iter 276/469 - loss 0.20520025 - samples/sec: 287.05 - lr: 0.100000\n",
      "2022-02-26 18:46:36,512 epoch 3 - iter 322/469 - loss 0.20420998 - samples/sec: 297.12 - lr: 0.100000\n",
      "2022-02-26 18:46:41,534 epoch 3 - iter 368/469 - loss 0.20347382 - samples/sec: 293.28 - lr: 0.100000\n",
      "2022-02-26 18:46:46,572 epoch 3 - iter 414/469 - loss 0.20215086 - samples/sec: 292.26 - lr: 0.100000\n",
      "2022-02-26 18:46:51,486 epoch 3 - iter 460/469 - loss 0.20226597 - samples/sec: 299.84 - lr: 0.100000\n",
      "2022-02-26 18:46:52,482 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 18:46:52,483 EPOCH 3 done: loss 0.2021 - lr 0.1000000\n",
      "2022-02-26 18:47:00,794 DEV : loss 0.11862613260746002 - f1-score (micro avg)  0.8214\n",
      "2022-02-26 18:47:00,882 BAD EPOCHS (no improvement): 0\n",
      "2022-02-26 18:47:00,883 saving best model\n",
      "2022-02-26 18:47:01,428 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 18:47:06,624 epoch 4 - iter 46/469 - loss 0.18800722 - samples/sec: 283.60 - lr: 0.100000\n",
      "2022-02-26 18:47:11,625 epoch 4 - iter 92/469 - loss 0.18411055 - samples/sec: 294.50 - lr: 0.100000\n",
      "2022-02-26 18:47:16,823 epoch 4 - iter 138/469 - loss 0.18778928 - samples/sec: 283.42 - lr: 0.100000\n",
      "2022-02-26 18:47:21,915 epoch 4 - iter 184/469 - loss 0.18429362 - samples/sec: 289.26 - lr: 0.100000\n",
      "2022-02-26 18:47:26,965 epoch 4 - iter 230/469 - loss 0.18400218 - samples/sec: 291.72 - lr: 0.100000\n",
      "2022-02-26 18:47:32,211 epoch 4 - iter 276/469 - loss 0.18305805 - samples/sec: 280.73 - lr: 0.100000\n",
      "2022-02-26 18:47:37,351 epoch 4 - iter 322/469 - loss 0.18234319 - samples/sec: 286.45 - lr: 0.100000\n",
      "2022-02-26 18:47:42,337 epoch 4 - iter 368/469 - loss 0.18266188 - samples/sec: 295.62 - lr: 0.100000\n",
      "2022-02-26 18:47:47,500 epoch 4 - iter 414/469 - loss 0.18207255 - samples/sec: 285.66 - lr: 0.100000\n",
      "2022-02-26 18:47:52,702 epoch 4 - iter 460/469 - loss 0.18234344 - samples/sec: 283.04 - lr: 0.100000\n",
      "2022-02-26 18:47:53,680 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 18:47:53,681 EPOCH 4 done: loss 0.1824 - lr 0.1000000\n",
      "2022-02-26 18:48:01,259 DEV : loss 0.10349219292402267 - f1-score (micro avg)  0.8343\n",
      "2022-02-26 18:48:01,349 BAD EPOCHS (no improvement): 0\n",
      "2022-02-26 18:48:01,350 saving best model\n",
      "2022-02-26 18:48:01,877 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 18:48:06,883 epoch 5 - iter 46/469 - loss 0.18025285 - samples/sec: 294.27 - lr: 0.100000\n",
      "2022-02-26 18:48:11,852 epoch 5 - iter 92/469 - loss 0.18071132 - samples/sec: 296.46 - lr: 0.100000\n",
      "2022-02-26 18:48:16,895 epoch 5 - iter 138/469 - loss 0.17921840 - samples/sec: 292.06 - lr: 0.100000\n",
      "2022-02-26 18:48:22,005 epoch 5 - iter 184/469 - loss 0.17488204 - samples/sec: 288.36 - lr: 0.100000\n",
      "2022-02-26 18:48:27,053 epoch 5 - iter 230/469 - loss 0.17442970 - samples/sec: 291.72 - lr: 0.100000\n",
      "2022-02-26 18:48:31,918 epoch 5 - iter 276/469 - loss 0.17407942 - samples/sec: 302.93 - lr: 0.100000\n",
      "2022-02-26 18:48:36,863 epoch 5 - iter 322/469 - loss 0.17422505 - samples/sec: 297.74 - lr: 0.100000\n",
      "2022-02-26 18:48:42,008 epoch 5 - iter 368/469 - loss 0.17363428 - samples/sec: 286.31 - lr: 0.100000\n",
      "2022-02-26 18:48:47,212 epoch 5 - iter 414/469 - loss 0.17281436 - samples/sec: 283.19 - lr: 0.100000\n",
      "2022-02-26 18:48:52,116 epoch 5 - iter 460/469 - loss 0.17132713 - samples/sec: 300.30 - lr: 0.100000\n",
      "2022-02-26 18:48:53,009 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 18:48:53,010 EPOCH 5 done: loss 0.1713 - lr 0.1000000\n",
      "2022-02-26 18:49:00,538 DEV : loss 0.0900411605834961 - f1-score (micro avg)  0.8522\n",
      "2022-02-26 18:49:00,631 BAD EPOCHS (no improvement): 0\n",
      "2022-02-26 18:49:00,633 saving best model\n",
      "2022-02-26 18:49:01,197 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 18:49:06,493 epoch 6 - iter 46/469 - loss 0.16180023 - samples/sec: 278.20 - lr: 0.100000\n",
      "2022-02-26 18:49:11,820 epoch 6 - iter 92/469 - loss 0.16077066 - samples/sec: 276.63 - lr: 0.100000\n",
      "2022-02-26 18:49:16,884 epoch 6 - iter 138/469 - loss 0.16262778 - samples/sec: 290.81 - lr: 0.100000\n",
      "2022-02-26 18:49:22,023 epoch 6 - iter 184/469 - loss 0.16268406 - samples/sec: 286.61 - lr: 0.100000\n",
      "2022-02-26 18:49:27,150 epoch 6 - iter 230/469 - loss 0.16395590 - samples/sec: 287.52 - lr: 0.100000\n",
      "2022-02-26 18:49:32,261 epoch 6 - iter 276/469 - loss 0.16490982 - samples/sec: 288.32 - lr: 0.100000\n",
      "2022-02-26 18:49:37,256 epoch 6 - iter 322/469 - loss 0.16294044 - samples/sec: 294.93 - lr: 0.100000\n",
      "2022-02-26 18:49:42,295 epoch 6 - iter 368/469 - loss 0.16331344 - samples/sec: 292.17 - lr: 0.100000\n",
      "2022-02-26 18:49:47,442 epoch 6 - iter 414/469 - loss 0.16203815 - samples/sec: 286.04 - lr: 0.100000\n",
      "2022-02-26 18:49:52,328 epoch 6 - iter 460/469 - loss 0.16181485 - samples/sec: 301.43 - lr: 0.100000\n",
      "2022-02-26 18:49:53,218 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 18:49:53,219 EPOCH 6 done: loss 0.1622 - lr 0.1000000\n",
      "2022-02-26 18:50:01,701 DEV : loss 0.09062226861715317 - f1-score (micro avg)  0.8597\n",
      "2022-02-26 18:50:01,793 BAD EPOCHS (no improvement): 0\n",
      "2022-02-26 18:50:01,794 saving best model\n",
      "2022-02-26 18:50:02,375 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 18:50:07,526 epoch 7 - iter 46/469 - loss 0.15165665 - samples/sec: 286.09 - lr: 0.100000\n",
      "2022-02-26 18:50:12,669 epoch 7 - iter 92/469 - loss 0.15305650 - samples/sec: 286.41 - lr: 0.100000\n",
      "2022-02-26 18:50:17,730 epoch 7 - iter 138/469 - loss 0.15443651 - samples/sec: 291.09 - lr: 0.100000\n",
      "2022-02-26 18:50:22,780 epoch 7 - iter 184/469 - loss 0.15541619 - samples/sec: 291.65 - lr: 0.100000\n",
      "2022-02-26 18:50:27,867 epoch 7 - iter 230/469 - loss 0.15613146 - samples/sec: 289.77 - lr: 0.100000\n",
      "2022-02-26 18:50:33,057 epoch 7 - iter 276/469 - loss 0.15645036 - samples/sec: 283.75 - lr: 0.100000\n",
      "2022-02-26 18:50:37,941 epoch 7 - iter 322/469 - loss 0.15597725 - samples/sec: 301.55 - lr: 0.100000\n",
      "2022-02-26 18:50:42,890 epoch 7 - iter 368/469 - loss 0.15650904 - samples/sec: 297.62 - lr: 0.100000\n",
      "2022-02-26 18:50:48,059 epoch 7 - iter 414/469 - loss 0.15507749 - samples/sec: 284.96 - lr: 0.100000\n",
      "2022-02-26 18:50:53,172 epoch 7 - iter 460/469 - loss 0.15497812 - samples/sec: 288.07 - lr: 0.100000\n",
      "2022-02-26 18:50:54,123 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 18:50:54,123 EPOCH 7 done: loss 0.1552 - lr 0.1000000\n",
      "2022-02-26 18:51:01,709 DEV : loss 0.08533807098865509 - f1-score (micro avg)  0.8629\n",
      "2022-02-26 18:51:01,803 BAD EPOCHS (no improvement): 0\n",
      "2022-02-26 18:51:01,805 saving best model\n",
      "2022-02-26 18:51:02,422 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 18:51:07,509 epoch 8 - iter 46/469 - loss 0.14546098 - samples/sec: 289.95 - lr: 0.100000\n",
      "2022-02-26 18:51:12,614 epoch 8 - iter 92/469 - loss 0.14918894 - samples/sec: 288.50 - lr: 0.100000\n",
      "2022-02-26 18:51:17,605 epoch 8 - iter 138/469 - loss 0.14711273 - samples/sec: 295.06 - lr: 0.100000\n",
      "2022-02-26 18:51:22,670 epoch 8 - iter 184/469 - loss 0.15048762 - samples/sec: 290.77 - lr: 0.100000\n",
      "2022-02-26 18:51:27,699 epoch 8 - iter 230/469 - loss 0.14867770 - samples/sec: 292.93 - lr: 0.100000\n",
      "2022-02-26 18:51:32,802 epoch 8 - iter 276/469 - loss 0.15046644 - samples/sec: 288.53 - lr: 0.100000\n",
      "2022-02-26 18:51:37,908 epoch 8 - iter 322/469 - loss 0.14904484 - samples/sec: 288.66 - lr: 0.100000\n",
      "2022-02-26 18:51:43,085 epoch 8 - iter 368/469 - loss 0.14975998 - samples/sec: 284.45 - lr: 0.100000\n",
      "2022-02-26 18:51:48,129 epoch 8 - iter 414/469 - loss 0.14903553 - samples/sec: 292.25 - lr: 0.100000\n",
      "2022-02-26 18:51:53,146 epoch 8 - iter 460/469 - loss 0.14908173 - samples/sec: 293.65 - lr: 0.100000\n",
      "2022-02-26 18:51:54,105 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 18:51:54,105 EPOCH 8 done: loss 0.1490 - lr 0.1000000\n",
      "2022-02-26 18:52:02,504 DEV : loss 0.08496016263961792 - f1-score (micro avg)  0.8687\n",
      "2022-02-26 18:52:02,594 BAD EPOCHS (no improvement): 0\n",
      "2022-02-26 18:52:02,596 saving best model\n",
      "2022-02-26 18:52:03,136 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 18:52:08,449 epoch 9 - iter 46/469 - loss 0.15294844 - samples/sec: 277.31 - lr: 0.100000\n",
      "2022-02-26 18:52:13,566 epoch 9 - iter 92/469 - loss 0.14853619 - samples/sec: 287.90 - lr: 0.100000\n",
      "2022-02-26 18:52:18,613 epoch 9 - iter 138/469 - loss 0.14672573 - samples/sec: 291.95 - lr: 0.100000\n",
      "2022-02-26 18:52:23,774 epoch 9 - iter 184/469 - loss 0.14789890 - samples/sec: 285.32 - lr: 0.100000\n",
      "2022-02-26 18:52:28,827 epoch 9 - iter 230/469 - loss 0.14734808 - samples/sec: 291.45 - lr: 0.100000\n",
      "2022-02-26 18:52:33,818 epoch 9 - iter 276/469 - loss 0.14498115 - samples/sec: 295.19 - lr: 0.100000\n",
      "2022-02-26 18:52:39,040 epoch 9 - iter 322/469 - loss 0.14474554 - samples/sec: 282.15 - lr: 0.100000\n",
      "2022-02-26 18:52:44,268 epoch 9 - iter 368/469 - loss 0.14568826 - samples/sec: 281.67 - lr: 0.100000\n",
      "2022-02-26 18:52:49,370 epoch 9 - iter 414/469 - loss 0.14569525 - samples/sec: 288.85 - lr: 0.100000\n",
      "2022-02-26 18:52:54,473 epoch 9 - iter 460/469 - loss 0.14586690 - samples/sec: 288.69 - lr: 0.100000\n",
      "2022-02-26 18:52:55,456 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 18:52:55,456 EPOCH 9 done: loss 0.1461 - lr 0.1000000\n",
      "2022-02-26 18:53:02,916 DEV : loss 0.07855367660522461 - f1-score (micro avg)  0.8715\n",
      "2022-02-26 18:53:03,004 BAD EPOCHS (no improvement): 0\n",
      "2022-02-26 18:53:03,005 saving best model\n",
      "2022-02-26 18:53:03,560 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 18:53:08,769 epoch 10 - iter 46/469 - loss 0.13820443 - samples/sec: 282.77 - lr: 0.100000\n",
      "2022-02-26 18:53:13,799 epoch 10 - iter 92/469 - loss 0.13700440 - samples/sec: 292.97 - lr: 0.100000\n",
      "2022-02-26 18:53:19,011 epoch 10 - iter 138/469 - loss 0.13628896 - samples/sec: 282.52 - lr: 0.100000\n",
      "2022-02-26 18:53:24,065 epoch 10 - iter 184/469 - loss 0.13963457 - samples/sec: 291.45 - lr: 0.100000\n",
      "2022-02-26 18:53:29,245 epoch 10 - iter 230/469 - loss 0.14019562 - samples/sec: 284.32 - lr: 0.100000\n",
      "2022-02-26 18:53:34,388 epoch 10 - iter 276/469 - loss 0.14145253 - samples/sec: 286.44 - lr: 0.100000\n",
      "2022-02-26 18:53:39,573 epoch 10 - iter 322/469 - loss 0.14168130 - samples/sec: 284.09 - lr: 0.100000\n",
      "2022-02-26 18:53:44,773 epoch 10 - iter 368/469 - loss 0.14196348 - samples/sec: 283.24 - lr: 0.100000\n",
      "2022-02-26 18:53:49,884 epoch 10 - iter 414/469 - loss 0.14216585 - samples/sec: 288.28 - lr: 0.100000\n",
      "2022-02-26 18:53:55,098 epoch 10 - iter 460/469 - loss 0.14232812 - samples/sec: 282.44 - lr: 0.100000\n",
      "2022-02-26 18:53:56,044 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 18:53:56,045 EPOCH 10 done: loss 0.1423 - lr 0.1000000\n",
      "2022-02-26 18:54:03,515 DEV : loss 0.07863521575927734 - f1-score (micro avg)  0.8771\n",
      "2022-02-26 18:54:03,605 BAD EPOCHS (no improvement): 0\n",
      "2022-02-26 18:54:03,606 saving best model\n",
      "2022-02-26 18:54:04,179 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 18:54:09,303 epoch 11 - iter 46/469 - loss 0.13565548 - samples/sec: 287.61 - lr: 0.100000\n",
      "2022-02-26 18:54:14,471 epoch 11 - iter 92/469 - loss 0.13748423 - samples/sec: 285.04 - lr: 0.100000\n",
      "2022-02-26 18:54:19,629 epoch 11 - iter 138/469 - loss 0.13688790 - samples/sec: 285.66 - lr: 0.100000\n",
      "2022-02-26 18:54:24,766 epoch 11 - iter 184/469 - loss 0.13857176 - samples/sec: 286.81 - lr: 0.100000\n",
      "2022-02-26 18:54:29,677 epoch 11 - iter 230/469 - loss 0.14046846 - samples/sec: 299.89 - lr: 0.100000\n",
      "2022-02-26 18:54:34,996 epoch 11 - iter 276/469 - loss 0.14027473 - samples/sec: 277.00 - lr: 0.100000\n",
      "2022-02-26 18:54:40,213 epoch 11 - iter 322/469 - loss 0.13977229 - samples/sec: 282.40 - lr: 0.100000\n",
      "2022-02-26 18:54:45,552 epoch 11 - iter 368/469 - loss 0.14025966 - samples/sec: 275.99 - lr: 0.100000\n",
      "2022-02-26 18:54:50,732 epoch 11 - iter 414/469 - loss 0.13969901 - samples/sec: 284.28 - lr: 0.100000\n",
      "2022-02-26 18:54:55,857 epoch 11 - iter 460/469 - loss 0.13904589 - samples/sec: 287.52 - lr: 0.100000\n",
      "2022-02-26 18:54:56,749 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 18:54:56,750 EPOCH 11 done: loss 0.1387 - lr 0.1000000\n",
      "2022-02-26 18:55:05,255 DEV : loss 0.07481548935174942 - f1-score (micro avg)  0.8814\n",
      "2022-02-26 18:55:05,345 BAD EPOCHS (no improvement): 0\n",
      "2022-02-26 18:55:05,346 saving best model\n",
      "2022-02-26 18:55:05,902 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 18:55:11,046 epoch 12 - iter 46/469 - loss 0.13850868 - samples/sec: 286.46 - lr: 0.100000\n",
      "2022-02-26 18:55:16,330 epoch 12 - iter 92/469 - loss 0.13908052 - samples/sec: 278.77 - lr: 0.100000\n",
      "2022-02-26 18:55:21,614 epoch 12 - iter 138/469 - loss 0.13893283 - samples/sec: 278.76 - lr: 0.100000\n",
      "2022-02-26 18:55:26,454 epoch 12 - iter 184/469 - loss 0.13635720 - samples/sec: 304.40 - lr: 0.100000\n",
      "2022-02-26 18:55:31,476 epoch 12 - iter 230/469 - loss 0.13733349 - samples/sec: 293.20 - lr: 0.100000\n",
      "2022-02-26 18:55:36,454 epoch 12 - iter 276/469 - loss 0.13652393 - samples/sec: 295.96 - lr: 0.100000\n",
      "2022-02-26 18:55:41,460 epoch 12 - iter 322/469 - loss 0.13644463 - samples/sec: 294.33 - lr: 0.100000\n",
      "2022-02-26 18:55:46,551 epoch 12 - iter 368/469 - loss 0.13552985 - samples/sec: 289.14 - lr: 0.100000\n",
      "2022-02-26 18:55:51,614 epoch 12 - iter 414/469 - loss 0.13575906 - samples/sec: 290.88 - lr: 0.100000\n",
      "2022-02-26 18:55:56,785 epoch 12 - iter 460/469 - loss 0.13533639 - samples/sec: 284.74 - lr: 0.100000\n",
      "2022-02-26 18:55:57,752 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 18:55:57,752 EPOCH 12 done: loss 0.1351 - lr 0.1000000\n",
      "2022-02-26 18:56:05,282 DEV : loss 0.07535504549741745 - f1-score (micro avg)  0.886\n",
      "2022-02-26 18:56:05,374 BAD EPOCHS (no improvement): 0\n",
      "2022-02-26 18:56:05,374 saving best model\n",
      "2022-02-26 18:56:05,941 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 18:56:11,132 epoch 13 - iter 46/469 - loss 0.13373128 - samples/sec: 283.87 - lr: 0.100000\n",
      "2022-02-26 18:56:16,353 epoch 13 - iter 92/469 - loss 0.13360359 - samples/sec: 282.22 - lr: 0.100000\n",
      "2022-02-26 18:56:21,366 epoch 13 - iter 138/469 - loss 0.13304077 - samples/sec: 293.66 - lr: 0.100000\n",
      "2022-02-26 18:56:26,587 epoch 13 - iter 184/469 - loss 0.13184933 - samples/sec: 282.31 - lr: 0.100000\n",
      "2022-02-26 18:56:31,693 epoch 13 - iter 230/469 - loss 0.13295096 - samples/sec: 288.51 - lr: 0.100000\n",
      "2022-02-26 18:56:36,624 epoch 13 - iter 276/469 - loss 0.13296534 - samples/sec: 298.75 - lr: 0.100000\n",
      "2022-02-26 18:56:41,564 epoch 13 - iter 322/469 - loss 0.13310877 - samples/sec: 298.20 - lr: 0.100000\n",
      "2022-02-26 18:56:46,706 epoch 13 - iter 368/469 - loss 0.13342743 - samples/sec: 286.57 - lr: 0.100000\n",
      "2022-02-26 18:56:51,636 epoch 13 - iter 414/469 - loss 0.13370607 - samples/sec: 298.65 - lr: 0.100000\n",
      "2022-02-26 18:56:56,615 epoch 13 - iter 460/469 - loss 0.13355330 - samples/sec: 295.84 - lr: 0.100000\n",
      "2022-02-26 18:56:57,503 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 18:56:57,504 EPOCH 13 done: loss 0.1334 - lr 0.1000000\n",
      "2022-02-26 18:57:04,891 DEV : loss 0.0737743228673935 - f1-score (micro avg)  0.883\n",
      "2022-02-26 18:57:04,983 BAD EPOCHS (no improvement): 1\n",
      "2022-02-26 18:57:04,984 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 18:57:10,055 epoch 14 - iter 46/469 - loss 0.12248235 - samples/sec: 290.60 - lr: 0.100000\n",
      "2022-02-26 18:57:15,106 epoch 14 - iter 92/469 - loss 0.12816731 - samples/sec: 291.72 - lr: 0.100000\n",
      "2022-02-26 18:57:20,186 epoch 14 - iter 138/469 - loss 0.12941569 - samples/sec: 289.87 - lr: 0.100000\n",
      "2022-02-26 18:57:25,141 epoch 14 - iter 184/469 - loss 0.13088218 - samples/sec: 297.21 - lr: 0.100000\n",
      "2022-02-26 18:57:30,243 epoch 14 - iter 230/469 - loss 0.13133314 - samples/sec: 288.74 - lr: 0.100000\n",
      "2022-02-26 18:57:35,211 epoch 14 - iter 276/469 - loss 0.13164610 - samples/sec: 296.68 - lr: 0.100000\n",
      "2022-02-26 18:57:40,326 epoch 14 - iter 322/469 - loss 0.13164644 - samples/sec: 287.88 - lr: 0.100000\n",
      "2022-02-26 18:57:45,576 epoch 14 - iter 368/469 - loss 0.13122486 - samples/sec: 280.56 - lr: 0.100000\n",
      "2022-02-26 18:57:50,603 epoch 14 - iter 414/469 - loss 0.13080348 - samples/sec: 293.02 - lr: 0.100000\n",
      "2022-02-26 18:57:55,653 epoch 14 - iter 460/469 - loss 0.13043707 - samples/sec: 291.69 - lr: 0.100000\n",
      "2022-02-26 18:57:56,609 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 18:57:56,610 EPOCH 14 done: loss 0.1303 - lr 0.1000000\n",
      "2022-02-26 18:58:05,194 DEV : loss 0.07060303539037704 - f1-score (micro avg)  0.8853\n",
      "2022-02-26 18:58:05,289 BAD EPOCHS (no improvement): 2\n",
      "2022-02-26 18:58:05,291 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 18:58:10,617 epoch 15 - iter 46/469 - loss 0.11781923 - samples/sec: 276.79 - lr: 0.100000\n",
      "2022-02-26 18:58:15,572 epoch 15 - iter 92/469 - loss 0.12239445 - samples/sec: 297.27 - lr: 0.100000\n",
      "2022-02-26 18:58:20,571 epoch 15 - iter 138/469 - loss 0.12430974 - samples/sec: 294.57 - lr: 0.100000\n",
      "2022-02-26 18:58:25,717 epoch 15 - iter 184/469 - loss 0.12656098 - samples/sec: 286.11 - lr: 0.100000\n",
      "2022-02-26 18:58:30,884 epoch 15 - iter 230/469 - loss 0.12669471 - samples/sec: 285.21 - lr: 0.100000\n",
      "2022-02-26 18:58:35,702 epoch 15 - iter 276/469 - loss 0.12763564 - samples/sec: 305.66 - lr: 0.100000\n",
      "2022-02-26 18:58:40,730 epoch 15 - iter 322/469 - loss 0.12833287 - samples/sec: 293.12 - lr: 0.100000\n",
      "2022-02-26 18:58:45,880 epoch 15 - iter 368/469 - loss 0.12813255 - samples/sec: 285.94 - lr: 0.100000\n",
      "2022-02-26 18:58:50,969 epoch 15 - iter 414/469 - loss 0.12834971 - samples/sec: 289.63 - lr: 0.100000\n",
      "2022-02-26 18:58:55,877 epoch 15 - iter 460/469 - loss 0.12901936 - samples/sec: 300.11 - lr: 0.100000\n",
      "2022-02-26 18:58:56,801 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 18:58:56,802 EPOCH 15 done: loss 0.1292 - lr 0.1000000\n",
      "2022-02-26 18:59:04,176 DEV : loss 0.07240357249975204 - f1-score (micro avg)  0.88\n",
      "2022-02-26 18:59:04,263 BAD EPOCHS (no improvement): 3\n",
      "2022-02-26 18:59:04,264 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 18:59:09,369 epoch 16 - iter 46/469 - loss 0.12244419 - samples/sec: 288.64 - lr: 0.100000\n",
      "2022-02-26 18:59:14,432 epoch 16 - iter 92/469 - loss 0.12224731 - samples/sec: 290.91 - lr: 0.100000\n",
      "2022-02-26 18:59:19,634 epoch 16 - iter 138/469 - loss 0.12399316 - samples/sec: 283.19 - lr: 0.100000\n",
      "2022-02-26 18:59:24,641 epoch 16 - iter 184/469 - loss 0.12236719 - samples/sec: 294.43 - lr: 0.100000\n",
      "2022-02-26 18:59:29,635 epoch 16 - iter 230/469 - loss 0.12251508 - samples/sec: 295.07 - lr: 0.100000\n",
      "2022-02-26 18:59:34,947 epoch 16 - iter 276/469 - loss 0.12364300 - samples/sec: 277.29 - lr: 0.100000\n",
      "2022-02-26 18:59:40,109 epoch 16 - iter 322/469 - loss 0.12415044 - samples/sec: 285.38 - lr: 0.100000\n",
      "2022-02-26 18:59:45,046 epoch 16 - iter 368/469 - loss 0.12479824 - samples/sec: 298.33 - lr: 0.100000\n",
      "2022-02-26 18:59:50,004 epoch 16 - iter 414/469 - loss 0.12571026 - samples/sec: 297.05 - lr: 0.100000\n",
      "2022-02-26 18:59:54,964 epoch 16 - iter 460/469 - loss 0.12606427 - samples/sec: 297.32 - lr: 0.100000\n",
      "2022-02-26 18:59:55,866 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 18:59:55,867 EPOCH 16 done: loss 0.1261 - lr 0.1000000\n",
      "2022-02-26 19:00:04,323 DEV : loss 0.06896398216485977 - f1-score (micro avg)  0.8919\n",
      "2022-02-26 19:00:04,419 BAD EPOCHS (no improvement): 0\n",
      "2022-02-26 19:00:04,421 saving best model\n",
      "2022-02-26 19:00:04,996 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:00:10,145 epoch 17 - iter 46/469 - loss 0.12554964 - samples/sec: 286.03 - lr: 0.100000\n",
      "2022-02-26 19:00:15,176 epoch 17 - iter 92/469 - loss 0.12675404 - samples/sec: 292.76 - lr: 0.100000\n",
      "2022-02-26 19:00:20,248 epoch 17 - iter 138/469 - loss 0.12567386 - samples/sec: 290.43 - lr: 0.100000\n",
      "2022-02-26 19:00:25,434 epoch 17 - iter 184/469 - loss 0.12564947 - samples/sec: 284.05 - lr: 0.100000\n",
      "2022-02-26 19:00:30,626 epoch 17 - iter 230/469 - loss 0.12526848 - samples/sec: 283.73 - lr: 0.100000\n",
      "2022-02-26 19:00:35,871 epoch 17 - iter 276/469 - loss 0.12439341 - samples/sec: 280.80 - lr: 0.100000\n",
      "2022-02-26 19:00:41,007 epoch 17 - iter 322/469 - loss 0.12440829 - samples/sec: 286.86 - lr: 0.100000\n",
      "2022-02-26 19:00:46,157 epoch 17 - iter 368/469 - loss 0.12460186 - samples/sec: 286.02 - lr: 0.100000\n",
      "2022-02-26 19:00:51,277 epoch 17 - iter 414/469 - loss 0.12428368 - samples/sec: 287.65 - lr: 0.100000\n",
      "2022-02-26 19:00:56,389 epoch 17 - iter 460/469 - loss 0.12451901 - samples/sec: 288.25 - lr: 0.100000\n",
      "2022-02-26 19:00:57,336 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:00:57,337 EPOCH 17 done: loss 0.1244 - lr 0.1000000\n",
      "2022-02-26 19:01:04,959 DEV : loss 0.06814901530742645 - f1-score (micro avg)  0.8896\n",
      "2022-02-26 19:01:05,049 BAD EPOCHS (no improvement): 1\n",
      "2022-02-26 19:01:05,050 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:01:10,134 epoch 18 - iter 46/469 - loss 0.12799702 - samples/sec: 289.79 - lr: 0.100000\n",
      "2022-02-26 19:01:15,183 epoch 18 - iter 92/469 - loss 0.12139940 - samples/sec: 291.81 - lr: 0.100000\n",
      "2022-02-26 19:01:20,482 epoch 18 - iter 138/469 - loss 0.12162778 - samples/sec: 278.11 - lr: 0.100000\n",
      "2022-02-26 19:01:25,508 epoch 18 - iter 184/469 - loss 0.12202490 - samples/sec: 292.94 - lr: 0.100000\n",
      "2022-02-26 19:01:30,478 epoch 18 - iter 230/469 - loss 0.12247096 - samples/sec: 296.44 - lr: 0.100000\n",
      "2022-02-26 19:01:35,519 epoch 18 - iter 276/469 - loss 0.12246269 - samples/sec: 292.27 - lr: 0.100000\n",
      "2022-02-26 19:01:40,779 epoch 18 - iter 322/469 - loss 0.12224701 - samples/sec: 280.11 - lr: 0.100000\n",
      "2022-02-26 19:01:45,728 epoch 18 - iter 368/469 - loss 0.12245397 - samples/sec: 297.64 - lr: 0.100000\n",
      "2022-02-26 19:01:50,913 epoch 18 - iter 414/469 - loss 0.12249623 - samples/sec: 284.26 - lr: 0.100000\n",
      "2022-02-26 19:01:56,076 epoch 18 - iter 460/469 - loss 0.12225261 - samples/sec: 285.33 - lr: 0.100000\n",
      "2022-02-26 19:01:57,042 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:01:57,043 EPOCH 18 done: loss 0.1223 - lr 0.1000000\n",
      "2022-02-26 19:02:04,725 DEV : loss 0.06858723610639572 - f1-score (micro avg)  0.8844\n",
      "2022-02-26 19:02:04,826 BAD EPOCHS (no improvement): 2\n",
      "2022-02-26 19:02:04,828 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:02:09,949 epoch 19 - iter 46/469 - loss 0.12042347 - samples/sec: 287.59 - lr: 0.100000\n",
      "2022-02-26 19:02:14,993 epoch 19 - iter 92/469 - loss 0.11927273 - samples/sec: 292.00 - lr: 0.100000\n",
      "2022-02-26 19:02:20,151 epoch 19 - iter 138/469 - loss 0.11969670 - samples/sec: 285.90 - lr: 0.100000\n",
      "2022-02-26 19:02:25,199 epoch 19 - iter 184/469 - loss 0.12007649 - samples/sec: 291.80 - lr: 0.100000\n",
      "2022-02-26 19:02:30,190 epoch 19 - iter 230/469 - loss 0.12277861 - samples/sec: 295.06 - lr: 0.100000\n",
      "2022-02-26 19:02:35,259 epoch 19 - iter 276/469 - loss 0.12194478 - samples/sec: 290.62 - lr: 0.100000\n",
      "2022-02-26 19:02:40,384 epoch 19 - iter 322/469 - loss 0.12272360 - samples/sec: 287.40 - lr: 0.100000\n",
      "2022-02-26 19:02:45,518 epoch 19 - iter 368/469 - loss 0.12267581 - samples/sec: 286.99 - lr: 0.100000\n",
      "2022-02-26 19:02:50,572 epoch 19 - iter 414/469 - loss 0.12421049 - samples/sec: 291.54 - lr: 0.100000\n",
      "2022-02-26 19:02:55,566 epoch 19 - iter 460/469 - loss 0.12359001 - samples/sec: 295.01 - lr: 0.100000\n",
      "2022-02-26 19:02:56,502 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:02:56,502 EPOCH 19 done: loss 0.1239 - lr 0.1000000\n",
      "2022-02-26 19:03:05,186 DEV : loss 0.0683007463812828 - f1-score (micro avg)  0.8914\n",
      "2022-02-26 19:03:05,282 BAD EPOCHS (no improvement): 3\n",
      "2022-02-26 19:03:05,284 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:03:10,413 epoch 20 - iter 46/469 - loss 0.12409238 - samples/sec: 287.32 - lr: 0.100000\n",
      "2022-02-26 19:03:15,613 epoch 20 - iter 92/469 - loss 0.12142775 - samples/sec: 283.24 - lr: 0.100000\n",
      "2022-02-26 19:03:20,717 epoch 20 - iter 138/469 - loss 0.12342923 - samples/sec: 288.50 - lr: 0.100000\n",
      "2022-02-26 19:03:25,632 epoch 20 - iter 184/469 - loss 0.12171106 - samples/sec: 299.66 - lr: 0.100000\n",
      "2022-02-26 19:03:30,787 epoch 20 - iter 230/469 - loss 0.12203308 - samples/sec: 285.85 - lr: 0.100000\n",
      "2022-02-26 19:03:35,858 epoch 20 - iter 276/469 - loss 0.12233561 - samples/sec: 290.65 - lr: 0.100000\n",
      "2022-02-26 19:03:40,902 epoch 20 - iter 322/469 - loss 0.12189910 - samples/sec: 291.85 - lr: 0.100000\n",
      "2022-02-26 19:03:45,749 epoch 20 - iter 368/469 - loss 0.12173785 - samples/sec: 304.12 - lr: 0.100000\n",
      "2022-02-26 19:03:50,660 epoch 20 - iter 414/469 - loss 0.12216740 - samples/sec: 299.98 - lr: 0.100000\n",
      "2022-02-26 19:03:55,655 epoch 20 - iter 460/469 - loss 0.12226120 - samples/sec: 294.95 - lr: 0.100000\n",
      "2022-02-26 19:03:56,659 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:03:56,659 EPOCH 20 done: loss 0.1224 - lr 0.1000000\n",
      "2022-02-26 19:04:04,121 DEV : loss 0.0666024312376976 - f1-score (micro avg)  0.8887\n",
      "Epoch    20: reducing learning rate of group 0 to 5.0000e-02.\n",
      "2022-02-26 19:04:04,212 BAD EPOCHS (no improvement): 4\n",
      "2022-02-26 19:04:04,213 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:04:09,456 epoch 21 - iter 46/469 - loss 0.11986793 - samples/sec: 281.17 - lr: 0.050000\n",
      "2022-02-26 19:04:14,678 epoch 21 - iter 92/469 - loss 0.11490674 - samples/sec: 282.16 - lr: 0.050000\n",
      "2022-02-26 19:04:19,939 epoch 21 - iter 138/469 - loss 0.11573026 - samples/sec: 279.88 - lr: 0.050000\n",
      "2022-02-26 19:04:25,046 epoch 21 - iter 184/469 - loss 0.11360680 - samples/sec: 288.32 - lr: 0.050000\n",
      "2022-02-26 19:04:30,198 epoch 21 - iter 230/469 - loss 0.11453824 - samples/sec: 285.95 - lr: 0.050000\n",
      "2022-02-26 19:04:35,246 epoch 21 - iter 276/469 - loss 0.11415217 - samples/sec: 292.01 - lr: 0.050000\n",
      "2022-02-26 19:04:40,402 epoch 21 - iter 322/469 - loss 0.11394016 - samples/sec: 285.57 - lr: 0.050000\n",
      "2022-02-26 19:04:45,576 epoch 21 - iter 368/469 - loss 0.11409324 - samples/sec: 284.73 - lr: 0.050000\n",
      "2022-02-26 19:04:50,606 epoch 21 - iter 414/469 - loss 0.11445374 - samples/sec: 292.82 - lr: 0.050000\n",
      "2022-02-26 19:04:55,888 epoch 21 - iter 460/469 - loss 0.11395902 - samples/sec: 278.73 - lr: 0.050000\n",
      "2022-02-26 19:04:56,877 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:04:56,877 EPOCH 21 done: loss 0.1143 - lr 0.0500000\n",
      "2022-02-26 19:05:04,342 DEV : loss 0.062365807592868805 - f1-score (micro avg)  0.896\n",
      "2022-02-26 19:05:04,437 BAD EPOCHS (no improvement): 0\n",
      "2022-02-26 19:05:04,438 saving best model\n",
      "2022-02-26 19:05:04,981 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:05:10,261 epoch 22 - iter 46/469 - loss 0.11463528 - samples/sec: 279.07 - lr: 0.050000\n",
      "2022-02-26 19:05:15,334 epoch 22 - iter 92/469 - loss 0.11146956 - samples/sec: 290.37 - lr: 0.050000\n",
      "2022-02-26 19:05:20,374 epoch 22 - iter 138/469 - loss 0.11248628 - samples/sec: 292.24 - lr: 0.050000\n",
      "2022-02-26 19:05:25,506 epoch 22 - iter 184/469 - loss 0.11169644 - samples/sec: 287.05 - lr: 0.050000\n",
      "2022-02-26 19:05:30,601 epoch 22 - iter 230/469 - loss 0.11151989 - samples/sec: 289.19 - lr: 0.050000\n",
      "2022-02-26 19:05:35,698 epoch 22 - iter 276/469 - loss 0.11108033 - samples/sec: 288.95 - lr: 0.050000\n",
      "2022-02-26 19:05:40,634 epoch 22 - iter 322/469 - loss 0.11067893 - samples/sec: 298.29 - lr: 0.050000\n",
      "2022-02-26 19:05:45,549 epoch 22 - iter 368/469 - loss 0.11119953 - samples/sec: 299.77 - lr: 0.050000\n",
      "2022-02-26 19:05:50,840 epoch 22 - iter 414/469 - loss 0.11155139 - samples/sec: 278.50 - lr: 0.050000\n",
      "2022-02-26 19:05:55,969 epoch 22 - iter 460/469 - loss 0.11218582 - samples/sec: 287.19 - lr: 0.050000\n",
      "2022-02-26 19:05:56,951 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:05:56,951 EPOCH 22 done: loss 0.1121 - lr 0.0500000\n",
      "2022-02-26 19:06:05,582 DEV : loss 0.06298784911632538 - f1-score (micro avg)  0.8969\n",
      "2022-02-26 19:06:05,675 BAD EPOCHS (no improvement): 0\n",
      "2022-02-26 19:06:05,676 saving best model\n",
      "2022-02-26 19:06:06,212 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:06:11,459 epoch 23 - iter 46/469 - loss 0.10887209 - samples/sec: 280.78 - lr: 0.050000\n",
      "2022-02-26 19:06:16,464 epoch 23 - iter 92/469 - loss 0.11062325 - samples/sec: 294.44 - lr: 0.050000\n",
      "2022-02-26 19:06:21,576 epoch 23 - iter 138/469 - loss 0.11076609 - samples/sec: 288.20 - lr: 0.050000\n",
      "2022-02-26 19:06:26,570 epoch 23 - iter 184/469 - loss 0.11144685 - samples/sec: 295.06 - lr: 0.050000\n",
      "2022-02-26 19:06:31,741 epoch 23 - iter 230/469 - loss 0.11172375 - samples/sec: 284.82 - lr: 0.050000\n",
      "2022-02-26 19:06:36,772 epoch 23 - iter 276/469 - loss 0.11081663 - samples/sec: 292.69 - lr: 0.050000\n",
      "2022-02-26 19:06:41,699 epoch 23 - iter 322/469 - loss 0.11172899 - samples/sec: 299.08 - lr: 0.050000\n",
      "2022-02-26 19:06:46,792 epoch 23 - iter 368/469 - loss 0.11200246 - samples/sec: 289.45 - lr: 0.050000\n",
      "2022-02-26 19:06:51,957 epoch 23 - iter 414/469 - loss 0.11218002 - samples/sec: 284.99 - lr: 0.050000\n",
      "2022-02-26 19:06:56,944 epoch 23 - iter 460/469 - loss 0.11153588 - samples/sec: 295.53 - lr: 0.050000\n",
      "2022-02-26 19:06:57,911 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:06:57,911 EPOCH 23 done: loss 0.1117 - lr 0.0500000\n",
      "2022-02-26 19:07:05,405 DEV : loss 0.06016507372260094 - f1-score (micro avg)  0.9001\n",
      "2022-02-26 19:07:05,494 BAD EPOCHS (no improvement): 0\n",
      "2022-02-26 19:07:05,496 saving best model\n",
      "2022-02-26 19:07:06,036 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:07:11,073 epoch 24 - iter 46/469 - loss 0.11355278 - samples/sec: 292.38 - lr: 0.050000\n",
      "2022-02-26 19:07:16,218 epoch 24 - iter 92/469 - loss 0.11116916 - samples/sec: 286.63 - lr: 0.050000\n",
      "2022-02-26 19:07:21,316 epoch 24 - iter 138/469 - loss 0.10991445 - samples/sec: 288.90 - lr: 0.050000\n",
      "2022-02-26 19:07:26,386 epoch 24 - iter 184/469 - loss 0.10804895 - samples/sec: 290.60 - lr: 0.050000\n",
      "2022-02-26 19:07:31,682 epoch 24 - iter 230/469 - loss 0.10717238 - samples/sec: 278.08 - lr: 0.050000\n",
      "2022-02-26 19:07:36,866 epoch 24 - iter 276/469 - loss 0.10666323 - samples/sec: 284.11 - lr: 0.050000\n",
      "2022-02-26 19:07:42,039 epoch 24 - iter 322/469 - loss 0.10686233 - samples/sec: 284.60 - lr: 0.050000\n",
      "2022-02-26 19:07:47,207 epoch 24 - iter 368/469 - loss 0.10771462 - samples/sec: 285.14 - lr: 0.050000\n",
      "2022-02-26 19:07:52,287 epoch 24 - iter 414/469 - loss 0.10749163 - samples/sec: 289.94 - lr: 0.050000\n",
      "2022-02-26 19:07:57,272 epoch 24 - iter 460/469 - loss 0.10656633 - samples/sec: 295.30 - lr: 0.050000\n",
      "2022-02-26 19:07:58,260 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:07:58,261 EPOCH 24 done: loss 0.1067 - lr 0.0500000\n",
      "2022-02-26 19:08:06,801 DEV : loss 0.06232693791389465 - f1-score (micro avg)  0.8966\n",
      "2022-02-26 19:08:06,895 BAD EPOCHS (no improvement): 1\n",
      "2022-02-26 19:08:06,896 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:08:11,892 epoch 25 - iter 46/469 - loss 0.10474040 - samples/sec: 294.80 - lr: 0.050000\n",
      "2022-02-26 19:08:16,949 epoch 25 - iter 92/469 - loss 0.10530168 - samples/sec: 291.18 - lr: 0.050000\n",
      "2022-02-26 19:08:21,943 epoch 25 - iter 138/469 - loss 0.10596749 - samples/sec: 294.94 - lr: 0.050000\n",
      "2022-02-26 19:08:26,993 epoch 25 - iter 184/469 - loss 0.10620740 - samples/sec: 291.62 - lr: 0.050000\n",
      "2022-02-26 19:08:32,135 epoch 25 - iter 230/469 - loss 0.10630262 - samples/sec: 286.77 - lr: 0.050000\n",
      "2022-02-26 19:08:37,350 epoch 25 - iter 276/469 - loss 0.10601159 - samples/sec: 282.56 - lr: 0.050000\n",
      "2022-02-26 19:08:42,548 epoch 25 - iter 322/469 - loss 0.10661525 - samples/sec: 283.40 - lr: 0.050000\n",
      "2022-02-26 19:08:47,517 epoch 25 - iter 368/469 - loss 0.10696249 - samples/sec: 296.46 - lr: 0.050000\n",
      "2022-02-26 19:08:52,621 epoch 25 - iter 414/469 - loss 0.10646332 - samples/sec: 288.68 - lr: 0.050000\n",
      "2022-02-26 19:08:57,930 epoch 25 - iter 460/469 - loss 0.10696836 - samples/sec: 277.44 - lr: 0.050000\n",
      "2022-02-26 19:08:58,882 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:08:58,883 EPOCH 25 done: loss 0.1070 - lr 0.0500000\n",
      "2022-02-26 19:09:06,266 DEV : loss 0.06100983917713165 - f1-score (micro avg)  0.8995\n",
      "2022-02-26 19:09:06,357 BAD EPOCHS (no improvement): 2\n",
      "2022-02-26 19:09:06,359 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:09:11,649 epoch 26 - iter 46/469 - loss 0.10762229 - samples/sec: 278.58 - lr: 0.050000\n",
      "2022-02-26 19:09:16,799 epoch 26 - iter 92/469 - loss 0.10836156 - samples/sec: 285.93 - lr: 0.050000\n",
      "2022-02-26 19:09:21,887 epoch 26 - iter 138/469 - loss 0.10900970 - samples/sec: 289.51 - lr: 0.050000\n",
      "2022-02-26 19:09:27,009 epoch 26 - iter 184/469 - loss 0.10567132 - samples/sec: 287.47 - lr: 0.050000\n",
      "2022-02-26 19:09:32,132 epoch 26 - iter 230/469 - loss 0.10588090 - samples/sec: 287.50 - lr: 0.050000\n",
      "2022-02-26 19:09:37,415 epoch 26 - iter 276/469 - loss 0.10654643 - samples/sec: 278.79 - lr: 0.050000\n",
      "2022-02-26 19:09:42,557 epoch 26 - iter 322/469 - loss 0.10596749 - samples/sec: 286.47 - lr: 0.050000\n",
      "2022-02-26 19:09:47,640 epoch 26 - iter 368/469 - loss 0.10579241 - samples/sec: 289.66 - lr: 0.050000\n",
      "2022-02-26 19:09:52,820 epoch 26 - iter 414/469 - loss 0.10649833 - samples/sec: 284.33 - lr: 0.050000\n",
      "2022-02-26 19:09:58,012 epoch 26 - iter 460/469 - loss 0.10714219 - samples/sec: 283.55 - lr: 0.050000\n",
      "2022-02-26 19:09:58,974 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:09:58,975 EPOCH 26 done: loss 0.1073 - lr 0.0500000\n",
      "2022-02-26 19:10:06,487 DEV : loss 0.06104003265500069 - f1-score (micro avg)  0.8991\n",
      "2022-02-26 19:10:06,585 BAD EPOCHS (no improvement): 3\n",
      "2022-02-26 19:10:06,587 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:10:11,736 epoch 27 - iter 46/469 - loss 0.10657181 - samples/sec: 286.03 - lr: 0.050000\n",
      "2022-02-26 19:10:16,976 epoch 27 - iter 92/469 - loss 0.10504521 - samples/sec: 281.13 - lr: 0.050000\n",
      "2022-02-26 19:10:21,975 epoch 27 - iter 138/469 - loss 0.10637594 - samples/sec: 294.65 - lr: 0.050000\n",
      "2022-02-26 19:10:27,053 epoch 27 - iter 184/469 - loss 0.10623436 - samples/sec: 289.99 - lr: 0.050000\n",
      "2022-02-26 19:10:32,240 epoch 27 - iter 230/469 - loss 0.10555275 - samples/sec: 284.16 - lr: 0.050000\n",
      "2022-02-26 19:10:37,371 epoch 27 - iter 276/469 - loss 0.10482120 - samples/sec: 287.05 - lr: 0.050000\n",
      "2022-02-26 19:10:42,490 epoch 27 - iter 322/469 - loss 0.10504204 - samples/sec: 287.76 - lr: 0.050000\n",
      "2022-02-26 19:10:47,673 epoch 27 - iter 368/469 - loss 0.10538043 - samples/sec: 284.59 - lr: 0.050000\n",
      "2022-02-26 19:10:52,652 epoch 27 - iter 414/469 - loss 0.10569904 - samples/sec: 295.88 - lr: 0.050000\n",
      "2022-02-26 19:10:57,771 epoch 27 - iter 460/469 - loss 0.10621752 - samples/sec: 287.98 - lr: 0.050000\n",
      "2022-02-26 19:10:58,691 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:10:58,691 EPOCH 27 done: loss 0.1063 - lr 0.0500000\n",
      "2022-02-26 19:11:07,224 DEV : loss 0.06024759262800217 - f1-score (micro avg)  0.9013\n",
      "2022-02-26 19:11:07,315 BAD EPOCHS (no improvement): 0\n",
      "2022-02-26 19:11:07,316 saving best model\n",
      "2022-02-26 19:11:07,859 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:11:12,936 epoch 28 - iter 46/469 - loss 0.10429849 - samples/sec: 290.07 - lr: 0.050000\n",
      "2022-02-26 19:11:18,255 epoch 28 - iter 92/469 - loss 0.10469923 - samples/sec: 276.86 - lr: 0.050000\n",
      "2022-02-26 19:11:23,479 epoch 28 - iter 138/469 - loss 0.10489432 - samples/sec: 282.12 - lr: 0.050000\n",
      "2022-02-26 19:11:28,542 epoch 28 - iter 184/469 - loss 0.10468124 - samples/sec: 291.05 - lr: 0.050000\n",
      "2022-02-26 19:11:33,776 epoch 28 - iter 230/469 - loss 0.10425181 - samples/sec: 281.59 - lr: 0.050000\n",
      "2022-02-26 19:11:38,919 epoch 28 - iter 276/469 - loss 0.10520266 - samples/sec: 286.43 - lr: 0.050000\n",
      "2022-02-26 19:11:44,107 epoch 28 - iter 322/469 - loss 0.10533507 - samples/sec: 284.12 - lr: 0.050000\n",
      "2022-02-26 19:11:49,341 epoch 28 - iter 368/469 - loss 0.10545891 - samples/sec: 281.46 - lr: 0.050000\n",
      "2022-02-26 19:11:54,366 epoch 28 - iter 414/469 - loss 0.10533714 - samples/sec: 293.23 - lr: 0.050000\n",
      "2022-02-26 19:11:59,398 epoch 28 - iter 460/469 - loss 0.10453797 - samples/sec: 292.73 - lr: 0.050000\n",
      "2022-02-26 19:12:00,422 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:12:00,422 EPOCH 28 done: loss 0.1044 - lr 0.0500000\n",
      "2022-02-26 19:12:07,922 DEV : loss 0.06117294356226921 - f1-score (micro avg)  0.9006\n",
      "2022-02-26 19:12:08,024 BAD EPOCHS (no improvement): 1\n",
      "2022-02-26 19:12:08,026 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:12:13,167 epoch 29 - iter 46/469 - loss 0.10986293 - samples/sec: 286.43 - lr: 0.050000\n",
      "2022-02-26 19:12:18,286 epoch 29 - iter 92/469 - loss 0.10733491 - samples/sec: 287.80 - lr: 0.050000\n",
      "2022-02-26 19:12:23,464 epoch 29 - iter 138/469 - loss 0.10566653 - samples/sec: 284.49 - lr: 0.050000\n",
      "2022-02-26 19:12:28,734 epoch 29 - iter 184/469 - loss 0.10697791 - samples/sec: 279.35 - lr: 0.050000\n",
      "2022-02-26 19:12:33,943 epoch 29 - iter 230/469 - loss 0.10787821 - samples/sec: 282.71 - lr: 0.050000\n",
      "2022-02-26 19:12:39,199 epoch 29 - iter 276/469 - loss 0.10539417 - samples/sec: 280.16 - lr: 0.050000\n",
      "2022-02-26 19:12:44,359 epoch 29 - iter 322/469 - loss 0.10518140 - samples/sec: 285.61 - lr: 0.050000\n",
      "2022-02-26 19:12:49,558 epoch 29 - iter 368/469 - loss 0.10475972 - samples/sec: 283.38 - lr: 0.050000\n",
      "2022-02-26 19:12:54,727 epoch 29 - iter 414/469 - loss 0.10486146 - samples/sec: 284.98 - lr: 0.050000\n",
      "2022-02-26 19:12:59,734 epoch 29 - iter 460/469 - loss 0.10462419 - samples/sec: 294.34 - lr: 0.050000\n",
      "2022-02-26 19:13:00,627 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:13:00,627 EPOCH 29 done: loss 0.1044 - lr 0.0500000\n",
      "2022-02-26 19:13:09,120 DEV : loss 0.0631057545542717 - f1-score (micro avg)  0.9017\n",
      "2022-02-26 19:13:09,220 BAD EPOCHS (no improvement): 0\n",
      "2022-02-26 19:13:09,222 saving best model\n",
      "2022-02-26 19:13:09,803 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:13:14,925 epoch 30 - iter 46/469 - loss 0.11013392 - samples/sec: 287.63 - lr: 0.050000\n",
      "2022-02-26 19:13:19,906 epoch 30 - iter 92/469 - loss 0.10977024 - samples/sec: 295.76 - lr: 0.050000\n",
      "2022-02-26 19:13:25,022 epoch 30 - iter 138/469 - loss 0.10672573 - samples/sec: 287.82 - lr: 0.050000\n",
      "2022-02-26 19:13:29,892 epoch 30 - iter 184/469 - loss 0.10362850 - samples/sec: 302.63 - lr: 0.050000\n",
      "2022-02-26 19:13:34,953 epoch 30 - iter 230/469 - loss 0.10263785 - samples/sec: 291.01 - lr: 0.050000\n",
      "2022-02-26 19:13:39,919 epoch 30 - iter 276/469 - loss 0.10285856 - samples/sec: 296.69 - lr: 0.050000\n",
      "2022-02-26 19:13:45,009 epoch 30 - iter 322/469 - loss 0.10260363 - samples/sec: 289.29 - lr: 0.050000\n",
      "2022-02-26 19:13:50,195 epoch 30 - iter 368/469 - loss 0.10400353 - samples/sec: 284.03 - lr: 0.050000\n",
      "2022-02-26 19:13:55,156 epoch 30 - iter 414/469 - loss 0.10449074 - samples/sec: 296.95 - lr: 0.050000\n",
      "2022-02-26 19:14:00,323 epoch 30 - iter 460/469 - loss 0.10398591 - samples/sec: 285.11 - lr: 0.050000\n",
      "2022-02-26 19:14:01,314 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:14:01,314 EPOCH 30 done: loss 0.1040 - lr 0.0500000\n",
      "2022-02-26 19:14:08,798 DEV : loss 0.06049352139234543 - f1-score (micro avg)  0.9005\n",
      "2022-02-26 19:14:08,887 BAD EPOCHS (no improvement): 1\n",
      "2022-02-26 19:14:08,888 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:14:14,017 epoch 31 - iter 46/469 - loss 0.10199211 - samples/sec: 287.13 - lr: 0.050000\n",
      "2022-02-26 19:14:19,049 epoch 31 - iter 92/469 - loss 0.09976278 - samples/sec: 292.84 - lr: 0.050000\n",
      "2022-02-26 19:14:24,392 epoch 31 - iter 138/469 - loss 0.10233922 - samples/sec: 275.83 - lr: 0.050000\n",
      "2022-02-26 19:14:29,452 epoch 31 - iter 184/469 - loss 0.10225059 - samples/sec: 291.11 - lr: 0.050000\n",
      "2022-02-26 19:14:34,686 epoch 31 - iter 230/469 - loss 0.10220284 - samples/sec: 281.47 - lr: 0.050000\n",
      "2022-02-26 19:14:39,796 epoch 31 - iter 276/469 - loss 0.10287412 - samples/sec: 288.23 - lr: 0.050000\n",
      "2022-02-26 19:14:44,941 epoch 31 - iter 322/469 - loss 0.10276963 - samples/sec: 286.42 - lr: 0.050000\n",
      "2022-02-26 19:14:50,017 epoch 31 - iter 368/469 - loss 0.10295896 - samples/sec: 290.19 - lr: 0.050000\n",
      "2022-02-26 19:14:55,094 epoch 31 - iter 414/469 - loss 0.10349250 - samples/sec: 290.24 - lr: 0.050000\n",
      "2022-02-26 19:15:00,337 epoch 31 - iter 460/469 - loss 0.10333290 - samples/sec: 281.05 - lr: 0.050000\n",
      "2022-02-26 19:15:01,312 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:15:01,313 EPOCH 31 done: loss 0.1034 - lr 0.0500000\n",
      "2022-02-26 19:15:08,774 DEV : loss 0.05934109538793564 - f1-score (micro avg)  0.9024\n",
      "2022-02-26 19:15:08,866 BAD EPOCHS (no improvement): 0\n",
      "2022-02-26 19:15:08,868 saving best model\n",
      "2022-02-26 19:15:09,441 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:15:14,720 epoch 32 - iter 46/469 - loss 0.10562159 - samples/sec: 278.97 - lr: 0.050000\n",
      "2022-02-26 19:15:19,801 epoch 32 - iter 92/469 - loss 0.10340948 - samples/sec: 289.76 - lr: 0.050000\n",
      "2022-02-26 19:15:24,972 epoch 32 - iter 138/469 - loss 0.10313680 - samples/sec: 284.99 - lr: 0.050000\n",
      "2022-02-26 19:15:30,084 epoch 32 - iter 184/469 - loss 0.10333502 - samples/sec: 288.03 - lr: 0.050000\n",
      "2022-02-26 19:15:35,266 epoch 32 - iter 230/469 - loss 0.10160470 - samples/sec: 284.24 - lr: 0.050000\n",
      "2022-02-26 19:15:40,365 epoch 32 - iter 276/469 - loss 0.10150649 - samples/sec: 288.98 - lr: 0.050000\n",
      "2022-02-26 19:15:45,408 epoch 32 - iter 322/469 - loss 0.10139037 - samples/sec: 292.15 - lr: 0.050000\n",
      "2022-02-26 19:15:50,378 epoch 32 - iter 368/469 - loss 0.10139068 - samples/sec: 296.67 - lr: 0.050000\n",
      "2022-02-26 19:15:55,511 epoch 32 - iter 414/469 - loss 0.10216905 - samples/sec: 286.94 - lr: 0.050000\n",
      "2022-02-26 19:16:00,578 epoch 32 - iter 460/469 - loss 0.10226973 - samples/sec: 290.79 - lr: 0.050000\n",
      "2022-02-26 19:16:01,528 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:16:01,528 EPOCH 32 done: loss 0.1022 - lr 0.0500000\n",
      "2022-02-26 19:16:09,978 DEV : loss 0.05822574719786644 - f1-score (micro avg)  0.904\n",
      "2022-02-26 19:16:10,074 BAD EPOCHS (no improvement): 0\n",
      "2022-02-26 19:16:10,076 saving best model\n",
      "2022-02-26 19:16:10,626 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:16:15,707 epoch 33 - iter 46/469 - loss 0.09863779 - samples/sec: 290.08 - lr: 0.050000\n",
      "2022-02-26 19:16:20,748 epoch 33 - iter 92/469 - loss 0.10013755 - samples/sec: 292.16 - lr: 0.050000\n",
      "2022-02-26 19:16:25,924 epoch 33 - iter 138/469 - loss 0.10104698 - samples/sec: 284.53 - lr: 0.050000\n",
      "2022-02-26 19:16:31,027 epoch 33 - iter 184/469 - loss 0.10228416 - samples/sec: 288.73 - lr: 0.050000\n",
      "2022-02-26 19:16:36,084 epoch 33 - iter 230/469 - loss 0.10192512 - samples/sec: 291.34 - lr: 0.050000\n",
      "2022-02-26 19:16:41,152 epoch 33 - iter 276/469 - loss 0.10187527 - samples/sec: 290.82 - lr: 0.050000\n",
      "2022-02-26 19:16:46,238 epoch 33 - iter 322/469 - loss 0.10196766 - samples/sec: 289.68 - lr: 0.050000\n",
      "2022-02-26 19:16:51,460 epoch 33 - iter 368/469 - loss 0.10129125 - samples/sec: 281.95 - lr: 0.050000\n",
      "2022-02-26 19:16:56,906 epoch 33 - iter 414/469 - loss 0.10119958 - samples/sec: 270.58 - lr: 0.050000\n",
      "2022-02-26 19:17:02,108 epoch 33 - iter 460/469 - loss 0.10131608 - samples/sec: 283.14 - lr: 0.050000\n",
      "2022-02-26 19:17:03,008 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:17:03,009 EPOCH 33 done: loss 0.1010 - lr 0.0500000\n",
      "2022-02-26 19:17:10,473 DEV : loss 0.058649539947509766 - f1-score (micro avg)  0.9047\n",
      "2022-02-26 19:17:10,564 BAD EPOCHS (no improvement): 0\n",
      "2022-02-26 19:17:10,566 saving best model\n",
      "2022-02-26 19:17:11,102 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:17:16,251 epoch 34 - iter 46/469 - loss 0.09984291 - samples/sec: 286.04 - lr: 0.050000\n",
      "2022-02-26 19:17:21,374 epoch 34 - iter 92/469 - loss 0.09388423 - samples/sec: 287.65 - lr: 0.050000\n",
      "2022-02-26 19:17:26,583 epoch 34 - iter 138/469 - loss 0.09712400 - samples/sec: 282.75 - lr: 0.050000\n",
      "2022-02-26 19:17:31,625 epoch 34 - iter 184/469 - loss 0.09975424 - samples/sec: 292.31 - lr: 0.050000\n",
      "2022-02-26 19:17:36,786 epoch 34 - iter 230/469 - loss 0.09929386 - samples/sec: 285.27 - lr: 0.050000\n",
      "2022-02-26 19:17:42,015 epoch 34 - iter 276/469 - loss 0.09982988 - samples/sec: 281.72 - lr: 0.050000\n",
      "2022-02-26 19:17:47,165 epoch 34 - iter 322/469 - loss 0.09899768 - samples/sec: 285.86 - lr: 0.050000\n",
      "2022-02-26 19:17:52,301 epoch 34 - iter 368/469 - loss 0.09962522 - samples/sec: 286.83 - lr: 0.050000\n",
      "2022-02-26 19:17:57,435 epoch 34 - iter 414/469 - loss 0.09952285 - samples/sec: 286.88 - lr: 0.050000\n",
      "2022-02-26 19:18:02,577 epoch 34 - iter 460/469 - loss 0.09954483 - samples/sec: 286.52 - lr: 0.050000\n",
      "2022-02-26 19:18:03,448 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:18:03,448 EPOCH 34 done: loss 0.0997 - lr 0.0500000\n",
      "2022-02-26 19:18:10,998 DEV : loss 0.05832327902317047 - f1-score (micro avg)  0.9027\n",
      "2022-02-26 19:18:11,087 BAD EPOCHS (no improvement): 1\n",
      "2022-02-26 19:18:11,088 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:18:16,239 epoch 35 - iter 46/469 - loss 0.10695843 - samples/sec: 285.98 - lr: 0.050000\n",
      "2022-02-26 19:18:21,400 epoch 35 - iter 92/469 - loss 0.10164259 - samples/sec: 285.38 - lr: 0.050000\n",
      "2022-02-26 19:18:26,615 epoch 35 - iter 138/469 - loss 0.10234638 - samples/sec: 282.43 - lr: 0.050000\n",
      "2022-02-26 19:18:31,688 epoch 35 - iter 184/469 - loss 0.09841431 - samples/sec: 290.54 - lr: 0.050000\n",
      "2022-02-26 19:18:36,756 epoch 35 - iter 230/469 - loss 0.09956954 - samples/sec: 290.70 - lr: 0.050000\n",
      "2022-02-26 19:18:41,876 epoch 35 - iter 276/469 - loss 0.09900834 - samples/sec: 287.82 - lr: 0.050000\n",
      "2022-02-26 19:18:47,037 epoch 35 - iter 322/469 - loss 0.09942478 - samples/sec: 285.38 - lr: 0.050000\n",
      "2022-02-26 19:18:52,211 epoch 35 - iter 368/469 - loss 0.09936783 - samples/sec: 284.70 - lr: 0.050000\n",
      "2022-02-26 19:18:57,480 epoch 35 - iter 414/469 - loss 0.10009540 - samples/sec: 279.57 - lr: 0.050000\n",
      "2022-02-26 19:19:02,710 epoch 35 - iter 460/469 - loss 0.10072498 - samples/sec: 281.80 - lr: 0.050000\n",
      "2022-02-26 19:19:03,701 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:19:03,701 EPOCH 35 done: loss 0.1009 - lr 0.0500000\n",
      "2022-02-26 19:19:12,168 DEV : loss 0.05824726074934006 - f1-score (micro avg)  0.9005\n",
      "2022-02-26 19:19:12,260 BAD EPOCHS (no improvement): 2\n",
      "2022-02-26 19:19:12,262 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:19:17,718 epoch 36 - iter 46/469 - loss 0.09117148 - samples/sec: 270.06 - lr: 0.050000\n",
      "2022-02-26 19:19:23,118 epoch 36 - iter 92/469 - loss 0.09652483 - samples/sec: 272.63 - lr: 0.050000\n",
      "2022-02-26 19:19:28,324 epoch 36 - iter 138/469 - loss 0.09844942 - samples/sec: 282.95 - lr: 0.050000\n",
      "2022-02-26 19:19:33,348 epoch 36 - iter 184/469 - loss 0.09926978 - samples/sec: 293.18 - lr: 0.050000\n",
      "2022-02-26 19:19:38,417 epoch 36 - iter 230/469 - loss 0.10015684 - samples/sec: 290.63 - lr: 0.050000\n",
      "2022-02-26 19:19:43,605 epoch 36 - iter 276/469 - loss 0.10077041 - samples/sec: 283.88 - lr: 0.050000\n",
      "2022-02-26 19:19:48,595 epoch 36 - iter 322/469 - loss 0.10011606 - samples/sec: 295.34 - lr: 0.050000\n",
      "2022-02-26 19:19:53,635 epoch 36 - iter 368/469 - loss 0.10037219 - samples/sec: 292.31 - lr: 0.050000\n",
      "2022-02-26 19:19:58,869 epoch 36 - iter 414/469 - loss 0.10029342 - samples/sec: 281.47 - lr: 0.050000\n",
      "2022-02-26 19:20:03,894 epoch 36 - iter 460/469 - loss 0.10015754 - samples/sec: 293.22 - lr: 0.050000\n",
      "2022-02-26 19:20:04,879 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:20:04,879 EPOCH 36 done: loss 0.1006 - lr 0.0500000\n",
      "2022-02-26 19:20:12,241 DEV : loss 0.058277636766433716 - f1-score (micro avg)  0.9037\n",
      "2022-02-26 19:20:12,334 BAD EPOCHS (no improvement): 3\n",
      "2022-02-26 19:20:12,335 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:20:17,510 epoch 37 - iter 46/469 - loss 0.09118844 - samples/sec: 284.78 - lr: 0.050000\n",
      "2022-02-26 19:20:22,568 epoch 37 - iter 92/469 - loss 0.09494789 - samples/sec: 291.15 - lr: 0.050000\n",
      "2022-02-26 19:20:27,671 epoch 37 - iter 138/469 - loss 0.09740430 - samples/sec: 288.61 - lr: 0.050000\n",
      "2022-02-26 19:20:32,962 epoch 37 - iter 184/469 - loss 0.09746595 - samples/sec: 278.53 - lr: 0.050000\n",
      "2022-02-26 19:20:38,187 epoch 37 - iter 230/469 - loss 0.09835506 - samples/sec: 282.02 - lr: 0.050000\n",
      "2022-02-26 19:20:43,361 epoch 37 - iter 276/469 - loss 0.09975038 - samples/sec: 284.64 - lr: 0.050000\n",
      "2022-02-26 19:20:48,478 epoch 37 - iter 322/469 - loss 0.10039728 - samples/sec: 288.01 - lr: 0.050000\n",
      "2022-02-26 19:20:53,491 epoch 37 - iter 368/469 - loss 0.10093091 - samples/sec: 293.84 - lr: 0.050000\n",
      "2022-02-26 19:20:58,767 epoch 37 - iter 414/469 - loss 0.10070769 - samples/sec: 279.02 - lr: 0.050000\n",
      "2022-02-26 19:21:03,929 epoch 37 - iter 460/469 - loss 0.10031296 - samples/sec: 285.17 - lr: 0.050000\n",
      "2022-02-26 19:21:04,870 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:21:04,871 EPOCH 37 done: loss 0.1001 - lr 0.0500000\n",
      "2022-02-26 19:21:15,775 DEV : loss 0.05910604074597359 - f1-score (micro avg)  0.9058\n",
      "2022-02-26 19:21:15,871 BAD EPOCHS (no improvement): 0\n",
      "2022-02-26 19:21:15,873 saving best model\n",
      "2022-02-26 19:21:16,508 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:21:21,729 epoch 38 - iter 46/469 - loss 0.10431918 - samples/sec: 282.17 - lr: 0.050000\n",
      "2022-02-26 19:21:26,893 epoch 38 - iter 92/469 - loss 0.10173670 - samples/sec: 285.11 - lr: 0.050000\n",
      "2022-02-26 19:21:32,018 epoch 38 - iter 138/469 - loss 0.09956381 - samples/sec: 287.53 - lr: 0.050000\n",
      "2022-02-26 19:21:37,136 epoch 38 - iter 184/469 - loss 0.09881928 - samples/sec: 287.78 - lr: 0.050000\n",
      "2022-02-26 19:21:42,254 epoch 38 - iter 230/469 - loss 0.09955309 - samples/sec: 287.70 - lr: 0.050000\n",
      "2022-02-26 19:21:47,450 epoch 38 - iter 276/469 - loss 0.09895026 - samples/sec: 283.64 - lr: 0.050000\n",
      "2022-02-26 19:21:52,479 epoch 38 - iter 322/469 - loss 0.09935528 - samples/sec: 292.94 - lr: 0.050000\n",
      "2022-02-26 19:21:57,709 epoch 38 - iter 368/469 - loss 0.09835345 - samples/sec: 281.68 - lr: 0.050000\n",
      "2022-02-26 19:22:02,854 epoch 38 - iter 414/469 - loss 0.09877079 - samples/sec: 286.15 - lr: 0.050000\n",
      "2022-02-26 19:22:07,909 epoch 38 - iter 460/469 - loss 0.09978688 - samples/sec: 291.36 - lr: 0.050000\n",
      "2022-02-26 19:22:08,783 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:22:08,784 EPOCH 38 done: loss 0.1000 - lr 0.0500000\n",
      "2022-02-26 19:22:16,266 DEV : loss 0.05760297551751137 - f1-score (micro avg)  0.9048\n",
      "2022-02-26 19:22:16,357 BAD EPOCHS (no improvement): 1\n",
      "2022-02-26 19:22:16,359 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:22:21,650 epoch 39 - iter 46/469 - loss 0.09139631 - samples/sec: 278.39 - lr: 0.050000\n",
      "2022-02-26 19:22:26,482 epoch 39 - iter 92/469 - loss 0.09524666 - samples/sec: 304.88 - lr: 0.050000\n",
      "2022-02-26 19:22:31,620 epoch 39 - iter 138/469 - loss 0.09811533 - samples/sec: 286.50 - lr: 0.050000\n",
      "2022-02-26 19:22:36,648 epoch 39 - iter 184/469 - loss 0.09813113 - samples/sec: 293.01 - lr: 0.050000\n",
      "2022-02-26 19:22:41,884 epoch 39 - iter 230/469 - loss 0.09862918 - samples/sec: 281.43 - lr: 0.050000\n",
      "2022-02-26 19:22:46,892 epoch 39 - iter 276/469 - loss 0.09818853 - samples/sec: 294.12 - lr: 0.050000\n",
      "2022-02-26 19:22:52,014 epoch 39 - iter 322/469 - loss 0.09835626 - samples/sec: 287.52 - lr: 0.050000\n",
      "2022-02-26 19:22:57,124 epoch 39 - iter 368/469 - loss 0.09875207 - samples/sec: 288.27 - lr: 0.050000\n",
      "2022-02-26 19:23:02,336 epoch 39 - iter 414/469 - loss 0.09812256 - samples/sec: 282.49 - lr: 0.050000\n",
      "2022-02-26 19:23:07,465 epoch 39 - iter 460/469 - loss 0.09874450 - samples/sec: 287.24 - lr: 0.050000\n",
      "2022-02-26 19:23:08,447 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:23:08,447 EPOCH 39 done: loss 0.0985 - lr 0.0500000\n",
      "2022-02-26 19:23:15,858 DEV : loss 0.06061139702796936 - f1-score (micro avg)  0.9046\n",
      "2022-02-26 19:23:15,947 BAD EPOCHS (no improvement): 2\n",
      "2022-02-26 19:23:15,949 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:23:21,028 epoch 40 - iter 46/469 - loss 0.09346291 - samples/sec: 290.16 - lr: 0.050000\n",
      "2022-02-26 19:23:25,993 epoch 40 - iter 92/469 - loss 0.09663855 - samples/sec: 296.79 - lr: 0.050000\n",
      "2022-02-26 19:23:31,093 epoch 40 - iter 138/469 - loss 0.09803603 - samples/sec: 288.70 - lr: 0.050000\n",
      "2022-02-26 19:23:36,127 epoch 40 - iter 184/469 - loss 0.10094636 - samples/sec: 292.61 - lr: 0.050000\n",
      "2022-02-26 19:23:41,356 epoch 40 - iter 230/469 - loss 0.09999148 - samples/sec: 281.65 - lr: 0.050000\n",
      "2022-02-26 19:23:46,368 epoch 40 - iter 276/469 - loss 0.09945067 - samples/sec: 293.79 - lr: 0.050000\n",
      "2022-02-26 19:23:51,404 epoch 40 - iter 322/469 - loss 0.09884237 - samples/sec: 292.31 - lr: 0.050000\n",
      "2022-02-26 19:23:56,532 epoch 40 - iter 368/469 - loss 0.09943341 - samples/sec: 287.33 - lr: 0.050000\n",
      "2022-02-26 19:24:01,642 epoch 40 - iter 414/469 - loss 0.09946247 - samples/sec: 288.33 - lr: 0.050000\n",
      "2022-02-26 19:24:06,849 epoch 40 - iter 460/469 - loss 0.09940674 - samples/sec: 282.97 - lr: 0.050000\n",
      "2022-02-26 19:24:07,804 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:24:07,804 EPOCH 40 done: loss 0.0992 - lr 0.0500000\n",
      "2022-02-26 19:24:16,368 DEV : loss 0.06070562079548836 - f1-score (micro avg)  0.9064\n",
      "2022-02-26 19:24:16,458 BAD EPOCHS (no improvement): 0\n",
      "2022-02-26 19:24:16,459 saving best model\n",
      "2022-02-26 19:24:17,045 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:24:22,412 epoch 41 - iter 46/469 - loss 0.09197646 - samples/sec: 274.64 - lr: 0.050000\n",
      "2022-02-26 19:24:27,473 epoch 41 - iter 92/469 - loss 0.09576550 - samples/sec: 291.04 - lr: 0.050000\n",
      "2022-02-26 19:24:32,564 epoch 41 - iter 138/469 - loss 0.09577098 - samples/sec: 289.51 - lr: 0.050000\n",
      "2022-02-26 19:24:37,749 epoch 41 - iter 184/469 - loss 0.09637516 - samples/sec: 284.01 - lr: 0.050000\n",
      "2022-02-26 19:24:42,979 epoch 41 - iter 230/469 - loss 0.09677752 - samples/sec: 281.58 - lr: 0.050000\n",
      "2022-02-26 19:24:48,198 epoch 41 - iter 276/469 - loss 0.09685932 - samples/sec: 282.21 - lr: 0.050000\n",
      "2022-02-26 19:24:53,325 epoch 41 - iter 322/469 - loss 0.09740899 - samples/sec: 287.50 - lr: 0.050000\n",
      "2022-02-26 19:24:58,472 epoch 41 - iter 368/469 - loss 0.09709623 - samples/sec: 286.12 - lr: 0.050000\n",
      "2022-02-26 19:25:03,602 epoch 41 - iter 414/469 - loss 0.09726199 - samples/sec: 287.39 - lr: 0.050000\n",
      "2022-02-26 19:25:08,676 epoch 41 - iter 460/469 - loss 0.09725854 - samples/sec: 290.16 - lr: 0.050000\n",
      "2022-02-26 19:25:09,606 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:25:09,607 EPOCH 41 done: loss 0.0972 - lr 0.0500000\n",
      "2022-02-26 19:25:17,103 DEV : loss 0.058877140283584595 - f1-score (micro avg)  0.905\n",
      "2022-02-26 19:25:17,195 BAD EPOCHS (no improvement): 1\n",
      "2022-02-26 19:25:17,196 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:25:22,451 epoch 42 - iter 46/469 - loss 0.09319225 - samples/sec: 280.39 - lr: 0.050000\n",
      "2022-02-26 19:25:27,659 epoch 42 - iter 92/469 - loss 0.09546433 - samples/sec: 282.73 - lr: 0.050000\n",
      "2022-02-26 19:25:32,644 epoch 42 - iter 138/469 - loss 0.09957585 - samples/sec: 295.70 - lr: 0.050000\n",
      "2022-02-26 19:25:37,791 epoch 42 - iter 184/469 - loss 0.09969392 - samples/sec: 286.41 - lr: 0.050000\n",
      "2022-02-26 19:25:42,884 epoch 42 - iter 230/469 - loss 0.10018493 - samples/sec: 289.25 - lr: 0.050000\n",
      "2022-02-26 19:25:47,763 epoch 42 - iter 276/469 - loss 0.09922628 - samples/sec: 301.94 - lr: 0.050000\n",
      "2022-02-26 19:25:52,946 epoch 42 - iter 322/469 - loss 0.09879178 - samples/sec: 284.11 - lr: 0.050000\n",
      "2022-02-26 19:25:58,134 epoch 42 - iter 368/469 - loss 0.09884572 - samples/sec: 283.73 - lr: 0.050000\n",
      "2022-02-26 19:26:03,702 epoch 42 - iter 414/469 - loss 0.09838503 - samples/sec: 264.52 - lr: 0.050000\n",
      "2022-02-26 19:26:08,900 epoch 42 - iter 460/469 - loss 0.09827547 - samples/sec: 283.34 - lr: 0.050000\n",
      "2022-02-26 19:26:09,971 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:26:09,971 EPOCH 42 done: loss 0.0983 - lr 0.0500000\n",
      "2022-02-26 19:26:19,299 DEV : loss 0.05620609223842621 - f1-score (micro avg)  0.9056\n",
      "2022-02-26 19:26:19,398 BAD EPOCHS (no improvement): 2\n",
      "2022-02-26 19:26:19,431 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:26:24,903 epoch 43 - iter 46/469 - loss 0.10184304 - samples/sec: 269.18 - lr: 0.050000\n",
      "2022-02-26 19:26:30,084 epoch 43 - iter 92/469 - loss 0.10083052 - samples/sec: 284.29 - lr: 0.050000\n",
      "2022-02-26 19:26:35,150 epoch 43 - iter 138/469 - loss 0.09745378 - samples/sec: 290.77 - lr: 0.050000\n",
      "2022-02-26 19:26:40,339 epoch 43 - iter 184/469 - loss 0.09663975 - samples/sec: 283.86 - lr: 0.050000\n",
      "2022-02-26 19:26:45,611 epoch 43 - iter 230/469 - loss 0.09477881 - samples/sec: 279.34 - lr: 0.050000\n",
      "2022-02-26 19:26:50,894 epoch 43 - iter 276/469 - loss 0.09492030 - samples/sec: 278.87 - lr: 0.050000\n",
      "2022-02-26 19:26:56,236 epoch 43 - iter 322/469 - loss 0.09477780 - samples/sec: 275.82 - lr: 0.050000\n",
      "2022-02-26 19:27:01,522 epoch 43 - iter 368/469 - loss 0.09461907 - samples/sec: 278.64 - lr: 0.050000\n",
      "2022-02-26 19:27:06,793 epoch 43 - iter 414/469 - loss 0.09518656 - samples/sec: 279.48 - lr: 0.050000\n",
      "2022-02-26 19:27:12,184 epoch 43 - iter 460/469 - loss 0.09535391 - samples/sec: 273.32 - lr: 0.050000\n",
      "2022-02-26 19:27:13,209 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:27:13,210 EPOCH 43 done: loss 0.0954 - lr 0.0500000\n",
      "2022-02-26 19:27:21,115 DEV : loss 0.056336432695388794 - f1-score (micro avg)  0.9056\n",
      "2022-02-26 19:27:21,249 BAD EPOCHS (no improvement): 3\n",
      "2022-02-26 19:27:21,257 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:27:27,094 epoch 44 - iter 46/469 - loss 0.09903459 - samples/sec: 252.36 - lr: 0.050000\n",
      "2022-02-26 19:27:32,708 epoch 44 - iter 92/469 - loss 0.09383520 - samples/sec: 262.48 - lr: 0.050000\n",
      "2022-02-26 19:27:38,309 epoch 44 - iter 138/469 - loss 0.09346895 - samples/sec: 262.95 - lr: 0.050000\n",
      "2022-02-26 19:27:43,953 epoch 44 - iter 184/469 - loss 0.09333086 - samples/sec: 260.86 - lr: 0.050000\n",
      "2022-02-26 19:27:49,124 epoch 44 - iter 230/469 - loss 0.09333217 - samples/sec: 284.93 - lr: 0.050000\n",
      "2022-02-26 19:27:54,212 epoch 44 - iter 276/469 - loss 0.09451403 - samples/sec: 289.63 - lr: 0.050000\n",
      "2022-02-26 19:27:59,396 epoch 44 - iter 322/469 - loss 0.09514752 - samples/sec: 284.29 - lr: 0.050000\n",
      "2022-02-26 19:28:04,418 epoch 44 - iter 368/469 - loss 0.09659113 - samples/sec: 293.39 - lr: 0.050000\n",
      "2022-02-26 19:28:09,575 epoch 44 - iter 414/469 - loss 0.09665568 - samples/sec: 285.54 - lr: 0.050000\n",
      "2022-02-26 19:28:14,681 epoch 44 - iter 460/469 - loss 0.09748188 - samples/sec: 288.49 - lr: 0.050000\n",
      "2022-02-26 19:28:15,672 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:28:15,673 EPOCH 44 done: loss 0.0976 - lr 0.0500000\n",
      "2022-02-26 19:28:23,210 DEV : loss 0.05865059420466423 - f1-score (micro avg)  0.906\n",
      "Epoch    44: reducing learning rate of group 0 to 2.5000e-02.\n",
      "2022-02-26 19:28:23,311 BAD EPOCHS (no improvement): 4\n",
      "2022-02-26 19:28:23,313 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:28:28,422 epoch 45 - iter 46/469 - loss 0.10458029 - samples/sec: 288.41 - lr: 0.025000\n",
      "2022-02-26 19:28:33,490 epoch 45 - iter 92/469 - loss 0.09962244 - samples/sec: 290.62 - lr: 0.025000\n",
      "2022-02-26 19:28:38,639 epoch 45 - iter 138/469 - loss 0.09668731 - samples/sec: 286.01 - lr: 0.025000\n",
      "2022-02-26 19:28:43,939 epoch 45 - iter 184/469 - loss 0.09851470 - samples/sec: 278.01 - lr: 0.025000\n",
      "2022-02-26 19:28:49,214 epoch 45 - iter 230/469 - loss 0.09683633 - samples/sec: 279.16 - lr: 0.025000\n",
      "2022-02-26 19:28:54,416 epoch 45 - iter 276/469 - loss 0.09561461 - samples/sec: 283.18 - lr: 0.025000\n",
      "2022-02-26 19:28:59,582 epoch 45 - iter 322/469 - loss 0.09545223 - samples/sec: 285.17 - lr: 0.025000\n",
      "2022-02-26 19:29:04,698 epoch 45 - iter 368/469 - loss 0.09566910 - samples/sec: 287.87 - lr: 0.025000\n",
      "2022-02-26 19:29:09,782 epoch 45 - iter 414/469 - loss 0.09526886 - samples/sec: 289.60 - lr: 0.025000\n",
      "2022-02-26 19:29:14,931 epoch 45 - iter 460/469 - loss 0.09499502 - samples/sec: 286.35 - lr: 0.025000\n",
      "2022-02-26 19:29:15,927 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:29:15,928 EPOCH 45 done: loss 0.0948 - lr 0.0250000\n",
      "2022-02-26 19:29:24,322 DEV : loss 0.0575561560690403 - f1-score (micro avg)  0.9062\n",
      "2022-02-26 19:29:24,411 BAD EPOCHS (no improvement): 1\n",
      "2022-02-26 19:29:24,412 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:29:29,752 epoch 46 - iter 46/469 - loss 0.09327866 - samples/sec: 276.06 - lr: 0.025000\n",
      "2022-02-26 19:29:35,528 epoch 46 - iter 92/469 - loss 0.09590265 - samples/sec: 255.00 - lr: 0.025000\n",
      "2022-02-26 19:29:41,087 epoch 46 - iter 138/469 - loss 0.09565535 - samples/sec: 264.89 - lr: 0.025000\n",
      "2022-02-26 19:29:46,437 epoch 46 - iter 184/469 - loss 0.09353998 - samples/sec: 275.26 - lr: 0.025000\n",
      "2022-02-26 19:29:51,879 epoch 46 - iter 230/469 - loss 0.09360734 - samples/sec: 270.56 - lr: 0.025000\n",
      "2022-02-26 19:29:57,179 epoch 46 - iter 276/469 - loss 0.09301320 - samples/sec: 277.85 - lr: 0.025000\n",
      "2022-02-26 19:30:02,373 epoch 46 - iter 322/469 - loss 0.09246134 - samples/sec: 283.62 - lr: 0.025000\n",
      "2022-02-26 19:30:07,857 epoch 46 - iter 368/469 - loss 0.09293161 - samples/sec: 268.70 - lr: 0.025000\n",
      "2022-02-26 19:30:13,198 epoch 46 - iter 414/469 - loss 0.09310465 - samples/sec: 275.82 - lr: 0.025000\n",
      "2022-02-26 19:30:18,510 epoch 46 - iter 460/469 - loss 0.09297559 - samples/sec: 277.38 - lr: 0.025000\n",
      "2022-02-26 19:30:19,470 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:30:19,470 EPOCH 46 done: loss 0.0928 - lr 0.0250000\n",
      "2022-02-26 19:30:27,104 DEV : loss 0.0572054497897625 - f1-score (micro avg)  0.9082\n",
      "2022-02-26 19:30:27,198 BAD EPOCHS (no improvement): 0\n",
      "2022-02-26 19:30:27,200 saving best model\n",
      "2022-02-26 19:30:27,802 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:30:33,093 epoch 47 - iter 46/469 - loss 0.08560336 - samples/sec: 278.49 - lr: 0.025000\n",
      "2022-02-26 19:30:38,199 epoch 47 - iter 92/469 - loss 0.09538082 - samples/sec: 288.47 - lr: 0.025000\n",
      "2022-02-26 19:30:43,714 epoch 47 - iter 138/469 - loss 0.09580526 - samples/sec: 267.09 - lr: 0.025000\n",
      "2022-02-26 19:30:49,154 epoch 47 - iter 184/469 - loss 0.09509771 - samples/sec: 270.82 - lr: 0.025000\n",
      "2022-02-26 19:30:54,698 epoch 47 - iter 230/469 - loss 0.09506050 - samples/sec: 265.91 - lr: 0.025000\n",
      "2022-02-26 19:31:00,009 epoch 47 - iter 276/469 - loss 0.09485250 - samples/sec: 277.36 - lr: 0.025000\n",
      "2022-02-26 19:31:05,479 epoch 47 - iter 322/469 - loss 0.09396591 - samples/sec: 269.48 - lr: 0.025000\n",
      "2022-02-26 19:31:11,027 epoch 47 - iter 368/469 - loss 0.09386903 - samples/sec: 265.57 - lr: 0.025000\n",
      "2022-02-26 19:31:16,296 epoch 47 - iter 414/469 - loss 0.09397893 - samples/sec: 279.54 - lr: 0.025000\n",
      "2022-02-26 19:31:21,711 epoch 47 - iter 460/469 - loss 0.09330409 - samples/sec: 272.06 - lr: 0.025000\n",
      "2022-02-26 19:31:22,745 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:31:22,746 EPOCH 47 done: loss 0.0936 - lr 0.0250000\n",
      "2022-02-26 19:31:30,609 DEV : loss 0.05670607462525368 - f1-score (micro avg)  0.9086\n",
      "2022-02-26 19:31:30,703 BAD EPOCHS (no improvement): 0\n",
      "2022-02-26 19:31:30,705 saving best model\n",
      "2022-02-26 19:31:31,323 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:31:36,776 epoch 48 - iter 46/469 - loss 0.08783570 - samples/sec: 270.08 - lr: 0.025000\n",
      "2022-02-26 19:31:41,940 epoch 48 - iter 92/469 - loss 0.08916460 - samples/sec: 285.48 - lr: 0.025000\n",
      "2022-02-26 19:31:47,070 epoch 48 - iter 138/469 - loss 0.08795175 - samples/sec: 287.26 - lr: 0.025000\n",
      "2022-02-26 19:31:52,344 epoch 48 - iter 184/469 - loss 0.09007825 - samples/sec: 279.64 - lr: 0.025000\n",
      "2022-02-26 19:31:57,712 epoch 48 - iter 230/469 - loss 0.09064407 - samples/sec: 274.24 - lr: 0.025000\n",
      "2022-02-26 19:32:02,887 epoch 48 - iter 276/469 - loss 0.09057761 - samples/sec: 284.60 - lr: 0.025000\n",
      "2022-02-26 19:32:08,109 epoch 48 - iter 322/469 - loss 0.09150367 - samples/sec: 281.99 - lr: 0.025000\n",
      "2022-02-26 19:32:13,628 epoch 48 - iter 368/469 - loss 0.09189708 - samples/sec: 266.80 - lr: 0.025000\n",
      "2022-02-26 19:32:18,956 epoch 48 - iter 414/469 - loss 0.09239985 - samples/sec: 276.50 - lr: 0.025000\n",
      "2022-02-26 19:32:24,178 epoch 48 - iter 460/469 - loss 0.09223021 - samples/sec: 282.31 - lr: 0.025000\n",
      "2022-02-26 19:32:25,196 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:32:25,196 EPOCH 48 done: loss 0.0923 - lr 0.0250000\n",
      "2022-02-26 19:32:34,018 DEV : loss 0.05651135742664337 - f1-score (micro avg)  0.908\n",
      "2022-02-26 19:32:34,112 BAD EPOCHS (no improvement): 1\n",
      "2022-02-26 19:32:34,113 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:32:39,359 epoch 49 - iter 46/469 - loss 0.09481155 - samples/sec: 281.05 - lr: 0.025000\n",
      "2022-02-26 19:32:44,574 epoch 49 - iter 92/469 - loss 0.09047147 - samples/sec: 282.44 - lr: 0.025000\n",
      "2022-02-26 19:32:49,668 epoch 49 - iter 138/469 - loss 0.09055511 - samples/sec: 289.16 - lr: 0.025000\n",
      "2022-02-26 19:32:54,986 epoch 49 - iter 184/469 - loss 0.08900751 - samples/sec: 276.91 - lr: 0.025000\n",
      "2022-02-26 19:33:00,139 epoch 49 - iter 230/469 - loss 0.08837836 - samples/sec: 285.76 - lr: 0.025000\n",
      "2022-02-26 19:33:05,239 epoch 49 - iter 276/469 - loss 0.08888259 - samples/sec: 288.82 - lr: 0.025000\n",
      "2022-02-26 19:33:10,111 epoch 49 - iter 322/469 - loss 0.08904870 - samples/sec: 302.33 - lr: 0.025000\n",
      "2022-02-26 19:33:15,403 epoch 49 - iter 368/469 - loss 0.08931898 - samples/sec: 278.18 - lr: 0.025000\n",
      "2022-02-26 19:33:20,525 epoch 49 - iter 414/469 - loss 0.09043705 - samples/sec: 287.66 - lr: 0.025000\n",
      "2022-02-26 19:33:25,647 epoch 49 - iter 460/469 - loss 0.09069575 - samples/sec: 287.64 - lr: 0.025000\n",
      "2022-02-26 19:33:26,651 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:33:26,651 EPOCH 49 done: loss 0.0904 - lr 0.0250000\n",
      "2022-02-26 19:33:34,246 DEV : loss 0.05630027502775192 - f1-score (micro avg)  0.9082\n",
      "2022-02-26 19:33:34,340 BAD EPOCHS (no improvement): 2\n",
      "2022-02-26 19:33:34,341 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:33:39,607 epoch 50 - iter 46/469 - loss 0.09996404 - samples/sec: 279.71 - lr: 0.025000\n",
      "2022-02-26 19:33:44,690 epoch 50 - iter 92/469 - loss 0.09093451 - samples/sec: 289.77 - lr: 0.025000\n",
      "2022-02-26 19:33:49,844 epoch 50 - iter 138/469 - loss 0.09237986 - samples/sec: 285.68 - lr: 0.025000\n",
      "2022-02-26 19:33:54,878 epoch 50 - iter 184/469 - loss 0.09257249 - samples/sec: 292.62 - lr: 0.025000\n",
      "2022-02-26 19:33:59,983 epoch 50 - iter 230/469 - loss 0.09338229 - samples/sec: 288.54 - lr: 0.025000\n",
      "2022-02-26 19:34:05,249 epoch 50 - iter 276/469 - loss 0.09334028 - samples/sec: 279.66 - lr: 0.025000\n",
      "2022-02-26 19:34:10,391 epoch 50 - iter 322/469 - loss 0.09220621 - samples/sec: 286.47 - lr: 0.025000\n",
      "2022-02-26 19:34:15,598 epoch 50 - iter 368/469 - loss 0.09284732 - samples/sec: 282.93 - lr: 0.025000\n",
      "2022-02-26 19:34:20,740 epoch 50 - iter 414/469 - loss 0.09193264 - samples/sec: 286.34 - lr: 0.025000\n",
      "2022-02-26 19:34:25,845 epoch 50 - iter 460/469 - loss 0.09128225 - samples/sec: 288.69 - lr: 0.025000\n",
      "2022-02-26 19:34:26,843 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:34:26,843 EPOCH 50 done: loss 0.0912 - lr 0.0250000\n",
      "2022-02-26 19:34:35,412 DEV : loss 0.057844679802656174 - f1-score (micro avg)  0.9075\n",
      "2022-02-26 19:34:35,503 BAD EPOCHS (no improvement): 3\n",
      "2022-02-26 19:34:35,504 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:34:40,806 epoch 51 - iter 46/469 - loss 0.09514262 - samples/sec: 277.98 - lr: 0.025000\n",
      "2022-02-26 19:34:45,833 epoch 51 - iter 92/469 - loss 0.09326608 - samples/sec: 293.07 - lr: 0.025000\n",
      "2022-02-26 19:34:51,046 epoch 51 - iter 138/469 - loss 0.09243389 - samples/sec: 282.53 - lr: 0.025000\n",
      "2022-02-26 19:34:56,195 epoch 51 - iter 184/469 - loss 0.09060149 - samples/sec: 286.07 - lr: 0.025000\n",
      "2022-02-26 19:35:01,360 epoch 51 - iter 230/469 - loss 0.09180196 - samples/sec: 285.29 - lr: 0.025000\n",
      "2022-02-26 19:35:06,397 epoch 51 - iter 276/469 - loss 0.09255811 - samples/sec: 292.36 - lr: 0.025000\n",
      "2022-02-26 19:35:11,642 epoch 51 - iter 322/469 - loss 0.09232774 - samples/sec: 280.98 - lr: 0.025000\n",
      "2022-02-26 19:35:16,807 epoch 51 - iter 368/469 - loss 0.09228584 - samples/sec: 285.19 - lr: 0.025000\n",
      "2022-02-26 19:35:22,184 epoch 51 - iter 414/469 - loss 0.09247675 - samples/sec: 274.01 - lr: 0.025000\n",
      "2022-02-26 19:35:27,163 epoch 51 - iter 460/469 - loss 0.09246362 - samples/sec: 296.09 - lr: 0.025000\n",
      "2022-02-26 19:35:28,094 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:35:28,095 EPOCH 51 done: loss 0.0924 - lr 0.0250000\n",
      "2022-02-26 19:35:35,651 DEV : loss 0.0561702586710453 - f1-score (micro avg)  0.9087\n",
      "2022-02-26 19:35:35,740 BAD EPOCHS (no improvement): 0\n",
      "2022-02-26 19:35:35,741 saving best model\n",
      "2022-02-26 19:35:36,312 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:35:41,572 epoch 52 - iter 46/469 - loss 0.09130832 - samples/sec: 280.18 - lr: 0.025000\n",
      "2022-02-26 19:35:46,701 epoch 52 - iter 92/469 - loss 0.08725650 - samples/sec: 287.39 - lr: 0.025000\n",
      "2022-02-26 19:35:51,935 epoch 52 - iter 138/469 - loss 0.08688256 - samples/sec: 281.36 - lr: 0.025000\n",
      "2022-02-26 19:35:57,070 epoch 52 - iter 184/469 - loss 0.08759272 - samples/sec: 286.99 - lr: 0.025000\n",
      "2022-02-26 19:36:02,286 epoch 52 - iter 230/469 - loss 0.08808936 - samples/sec: 282.46 - lr: 0.025000\n",
      "2022-02-26 19:36:07,649 epoch 52 - iter 276/469 - loss 0.08887119 - samples/sec: 274.67 - lr: 0.025000\n",
      "2022-02-26 19:36:12,806 epoch 52 - iter 322/469 - loss 0.08949760 - samples/sec: 285.60 - lr: 0.025000\n",
      "2022-02-26 19:36:18,108 epoch 52 - iter 368/469 - loss 0.08943028 - samples/sec: 277.85 - lr: 0.025000\n",
      "2022-02-26 19:36:23,179 epoch 52 - iter 414/469 - loss 0.09029963 - samples/sec: 290.53 - lr: 0.025000\n",
      "2022-02-26 19:36:28,331 epoch 52 - iter 460/469 - loss 0.09006264 - samples/sec: 285.81 - lr: 0.025000\n",
      "2022-02-26 19:36:29,269 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:36:29,270 EPOCH 52 done: loss 0.0899 - lr 0.0250000\n",
      "2022-02-26 19:36:36,766 DEV : loss 0.056106965988874435 - f1-score (micro avg)  0.9092\n",
      "2022-02-26 19:36:36,856 BAD EPOCHS (no improvement): 0\n",
      "2022-02-26 19:36:36,857 saving best model\n",
      "2022-02-26 19:36:37,439 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:36:42,568 epoch 53 - iter 46/469 - loss 0.09317523 - samples/sec: 287.32 - lr: 0.025000\n",
      "2022-02-26 19:36:47,679 epoch 53 - iter 92/469 - loss 0.09071515 - samples/sec: 288.15 - lr: 0.025000\n",
      "2022-02-26 19:36:52,930 epoch 53 - iter 138/469 - loss 0.09133014 - samples/sec: 280.43 - lr: 0.025000\n",
      "2022-02-26 19:36:58,049 epoch 53 - iter 184/469 - loss 0.09189626 - samples/sec: 287.80 - lr: 0.025000\n",
      "2022-02-26 19:37:03,141 epoch 53 - iter 230/469 - loss 0.09225494 - samples/sec: 289.21 - lr: 0.025000\n",
      "2022-02-26 19:37:08,291 epoch 53 - iter 276/469 - loss 0.09145037 - samples/sec: 286.00 - lr: 0.025000\n",
      "2022-02-26 19:37:13,399 epoch 53 - iter 322/469 - loss 0.09125357 - samples/sec: 288.34 - lr: 0.025000\n",
      "2022-02-26 19:37:18,669 epoch 53 - iter 368/469 - loss 0.09127871 - samples/sec: 279.53 - lr: 0.025000\n",
      "2022-02-26 19:37:23,647 epoch 53 - iter 414/469 - loss 0.09141664 - samples/sec: 296.01 - lr: 0.025000\n",
      "2022-02-26 19:37:28,831 epoch 53 - iter 460/469 - loss 0.09141238 - samples/sec: 284.27 - lr: 0.025000\n",
      "2022-02-26 19:37:29,790 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:37:29,790 EPOCH 53 done: loss 0.0915 - lr 0.0250000\n",
      "2022-02-26 19:37:38,313 DEV : loss 0.05659976974129677 - f1-score (micro avg)  0.9074\n",
      "2022-02-26 19:37:38,401 BAD EPOCHS (no improvement): 1\n",
      "2022-02-26 19:37:38,403 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:37:43,732 epoch 54 - iter 46/469 - loss 0.08732385 - samples/sec: 276.81 - lr: 0.025000\n",
      "2022-02-26 19:37:48,773 epoch 54 - iter 92/469 - loss 0.08514872 - samples/sec: 292.13 - lr: 0.025000\n",
      "2022-02-26 19:37:53,866 epoch 54 - iter 138/469 - loss 0.08775390 - samples/sec: 289.36 - lr: 0.025000\n",
      "2022-02-26 19:37:58,995 epoch 54 - iter 184/469 - loss 0.09054661 - samples/sec: 287.19 - lr: 0.025000\n",
      "2022-02-26 19:38:04,264 epoch 54 - iter 230/469 - loss 0.08923032 - samples/sec: 279.53 - lr: 0.025000\n",
      "2022-02-26 19:38:09,440 epoch 54 - iter 276/469 - loss 0.08902063 - samples/sec: 284.43 - lr: 0.025000\n",
      "2022-02-26 19:38:14,693 epoch 54 - iter 322/469 - loss 0.08885237 - samples/sec: 280.35 - lr: 0.025000\n",
      "2022-02-26 19:38:19,692 epoch 54 - iter 368/469 - loss 0.09025575 - samples/sec: 295.04 - lr: 0.025000\n",
      "2022-02-26 19:38:24,751 epoch 54 - iter 414/469 - loss 0.08995232 - samples/sec: 291.30 - lr: 0.025000\n",
      "2022-02-26 19:38:29,936 epoch 54 - iter 460/469 - loss 0.08956340 - samples/sec: 284.10 - lr: 0.025000\n",
      "2022-02-26 19:38:30,860 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:38:30,860 EPOCH 54 done: loss 0.0896 - lr 0.0250000\n",
      "2022-02-26 19:38:38,366 DEV : loss 0.05663695186376572 - f1-score (micro avg)  0.909\n",
      "2022-02-26 19:38:38,459 BAD EPOCHS (no improvement): 2\n",
      "2022-02-26 19:38:38,460 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:38:43,670 epoch 55 - iter 46/469 - loss 0.08924840 - samples/sec: 282.97 - lr: 0.025000\n",
      "2022-02-26 19:38:48,775 epoch 55 - iter 92/469 - loss 0.09010596 - samples/sec: 288.53 - lr: 0.025000\n",
      "2022-02-26 19:38:53,830 epoch 55 - iter 138/469 - loss 0.08994818 - samples/sec: 291.48 - lr: 0.025000\n",
      "2022-02-26 19:38:58,845 epoch 55 - iter 184/469 - loss 0.09074482 - samples/sec: 293.74 - lr: 0.025000\n",
      "2022-02-26 19:39:03,981 epoch 55 - iter 230/469 - loss 0.09117799 - samples/sec: 286.68 - lr: 0.025000\n",
      "2022-02-26 19:39:09,170 epoch 55 - iter 276/469 - loss 0.09102721 - samples/sec: 283.66 - lr: 0.025000\n",
      "2022-02-26 19:39:14,343 epoch 55 - iter 322/469 - loss 0.08956617 - samples/sec: 284.84 - lr: 0.025000\n",
      "2022-02-26 19:39:19,638 epoch 55 - iter 368/469 - loss 0.08950813 - samples/sec: 278.25 - lr: 0.025000\n",
      "2022-02-26 19:39:25,146 epoch 55 - iter 414/469 - loss 0.08901166 - samples/sec: 267.48 - lr: 0.025000\n",
      "2022-02-26 19:39:30,567 epoch 55 - iter 460/469 - loss 0.08932044 - samples/sec: 271.66 - lr: 0.025000\n",
      "2022-02-26 19:39:31,598 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:39:31,598 EPOCH 55 done: loss 0.0893 - lr 0.0250000\n",
      "2022-02-26 19:39:40,350 DEV : loss 0.05516654998064041 - f1-score (micro avg)  0.9094\n",
      "2022-02-26 19:39:40,441 BAD EPOCHS (no improvement): 0\n",
      "2022-02-26 19:39:40,442 saving best model\n",
      "2022-02-26 19:39:41,089 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:39:46,503 epoch 56 - iter 46/469 - loss 0.09203796 - samples/sec: 272.16 - lr: 0.025000\n",
      "2022-02-26 19:39:51,873 epoch 56 - iter 92/469 - loss 0.08839281 - samples/sec: 274.18 - lr: 0.025000\n",
      "2022-02-26 19:39:57,200 epoch 56 - iter 138/469 - loss 0.08937356 - samples/sec: 276.68 - lr: 0.025000\n",
      "2022-02-26 19:40:02,566 epoch 56 - iter 184/469 - loss 0.08889697 - samples/sec: 274.55 - lr: 0.025000\n",
      "2022-02-26 19:40:07,664 epoch 56 - iter 230/469 - loss 0.08908112 - samples/sec: 289.11 - lr: 0.025000\n",
      "2022-02-26 19:40:12,924 epoch 56 - iter 276/469 - loss 0.08977124 - samples/sec: 280.20 - lr: 0.025000\n",
      "2022-02-26 19:40:18,120 epoch 56 - iter 322/469 - loss 0.08965923 - samples/sec: 283.37 - lr: 0.025000\n",
      "2022-02-26 19:40:23,271 epoch 56 - iter 368/469 - loss 0.08966654 - samples/sec: 285.99 - lr: 0.025000\n",
      "2022-02-26 19:40:28,371 epoch 56 - iter 414/469 - loss 0.08976555 - samples/sec: 288.93 - lr: 0.025000\n",
      "2022-02-26 19:40:33,526 epoch 56 - iter 460/469 - loss 0.09029862 - samples/sec: 285.57 - lr: 0.025000\n",
      "2022-02-26 19:40:34,516 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:40:34,517 EPOCH 56 done: loss 0.0904 - lr 0.0250000\n",
      "2022-02-26 19:40:41,933 DEV : loss 0.05600135773420334 - f1-score (micro avg)  0.9075\n",
      "2022-02-26 19:40:42,023 BAD EPOCHS (no improvement): 1\n",
      "2022-02-26 19:40:42,024 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:40:47,026 epoch 57 - iter 46/469 - loss 0.08421169 - samples/sec: 294.64 - lr: 0.025000\n",
      "2022-02-26 19:40:52,120 epoch 57 - iter 92/469 - loss 0.08816012 - samples/sec: 289.07 - lr: 0.025000\n",
      "2022-02-26 19:40:57,237 epoch 57 - iter 138/469 - loss 0.08941983 - samples/sec: 287.84 - lr: 0.025000\n",
      "2022-02-26 19:41:02,433 epoch 57 - iter 184/469 - loss 0.08855461 - samples/sec: 283.42 - lr: 0.025000\n",
      "2022-02-26 19:41:07,644 epoch 57 - iter 230/469 - loss 0.08957606 - samples/sec: 282.59 - lr: 0.025000\n",
      "2022-02-26 19:41:12,818 epoch 57 - iter 276/469 - loss 0.08995114 - samples/sec: 284.67 - lr: 0.025000\n",
      "2022-02-26 19:41:18,037 epoch 57 - iter 322/469 - loss 0.08941703 - samples/sec: 282.22 - lr: 0.025000\n",
      "2022-02-26 19:41:23,384 epoch 57 - iter 368/469 - loss 0.08875674 - samples/sec: 275.51 - lr: 0.025000\n",
      "2022-02-26 19:41:28,766 epoch 57 - iter 414/469 - loss 0.08859913 - samples/sec: 273.56 - lr: 0.025000\n",
      "2022-02-26 19:41:34,143 epoch 57 - iter 460/469 - loss 0.08851748 - samples/sec: 274.29 - lr: 0.025000\n",
      "2022-02-26 19:41:35,172 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:41:35,172 EPOCH 57 done: loss 0.0884 - lr 0.0250000\n",
      "2022-02-26 19:41:43,240 DEV : loss 0.055711157619953156 - f1-score (micro avg)  0.9087\n",
      "2022-02-26 19:41:43,336 BAD EPOCHS (no improvement): 2\n",
      "2022-02-26 19:41:43,337 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:41:48,438 epoch 58 - iter 46/469 - loss 0.08621815 - samples/sec: 288.82 - lr: 0.025000\n",
      "2022-02-26 19:41:53,528 epoch 58 - iter 92/469 - loss 0.08669909 - samples/sec: 289.34 - lr: 0.025000\n",
      "2022-02-26 19:41:58,572 epoch 58 - iter 138/469 - loss 0.08696584 - samples/sec: 292.03 - lr: 0.025000\n",
      "2022-02-26 19:42:03,328 epoch 58 - iter 184/469 - loss 0.08803293 - samples/sec: 309.82 - lr: 0.025000\n",
      "2022-02-26 19:42:08,519 epoch 58 - iter 230/469 - loss 0.08799809 - samples/sec: 283.82 - lr: 0.025000\n",
      "2022-02-26 19:42:13,516 epoch 58 - iter 276/469 - loss 0.08880685 - samples/sec: 294.92 - lr: 0.025000\n",
      "2022-02-26 19:42:18,603 epoch 58 - iter 322/469 - loss 0.08947434 - samples/sec: 289.55 - lr: 0.025000\n",
      "2022-02-26 19:42:23,804 epoch 58 - iter 368/469 - loss 0.08968476 - samples/sec: 283.23 - lr: 0.025000\n",
      "2022-02-26 19:42:28,593 epoch 58 - iter 414/469 - loss 0.08963867 - samples/sec: 307.68 - lr: 0.025000\n",
      "2022-02-26 19:42:33,573 epoch 58 - iter 460/469 - loss 0.08935837 - samples/sec: 295.60 - lr: 0.025000\n",
      "2022-02-26 19:42:34,500 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:42:34,501 EPOCH 58 done: loss 0.0894 - lr 0.0250000\n",
      "2022-02-26 19:42:43,418 DEV : loss 0.05557380989193916 - f1-score (micro avg)  0.9109\n",
      "2022-02-26 19:42:43,519 BAD EPOCHS (no improvement): 0\n",
      "2022-02-26 19:42:43,521 saving best model\n",
      "2022-02-26 19:42:44,074 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:42:49,226 epoch 59 - iter 46/469 - loss 0.08603151 - samples/sec: 286.06 - lr: 0.025000\n",
      "2022-02-26 19:42:54,105 epoch 59 - iter 92/469 - loss 0.08812910 - samples/sec: 302.24 - lr: 0.025000\n",
      "2022-02-26 19:42:59,211 epoch 59 - iter 138/469 - loss 0.09004533 - samples/sec: 288.44 - lr: 0.025000\n",
      "2022-02-26 19:43:04,230 epoch 59 - iter 184/469 - loss 0.09034950 - samples/sec: 293.56 - lr: 0.025000\n",
      "2022-02-26 19:43:09,211 epoch 59 - iter 230/469 - loss 0.08953119 - samples/sec: 295.85 - lr: 0.025000\n",
      "2022-02-26 19:43:14,248 epoch 59 - iter 276/469 - loss 0.08886519 - samples/sec: 292.48 - lr: 0.025000\n",
      "2022-02-26 19:43:19,445 epoch 59 - iter 322/469 - loss 0.08886407 - samples/sec: 283.38 - lr: 0.025000\n",
      "2022-02-26 19:43:24,389 epoch 59 - iter 368/469 - loss 0.08913484 - samples/sec: 297.98 - lr: 0.025000\n",
      "2022-02-26 19:43:29,560 epoch 59 - iter 414/469 - loss 0.08894905 - samples/sec: 284.88 - lr: 0.025000\n",
      "2022-02-26 19:43:34,593 epoch 59 - iter 460/469 - loss 0.08947288 - samples/sec: 292.69 - lr: 0.025000\n",
      "2022-02-26 19:43:35,510 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:43:35,511 EPOCH 59 done: loss 0.0893 - lr 0.0250000\n",
      "2022-02-26 19:43:43,171 DEV : loss 0.0572945736348629 - f1-score (micro avg)  0.9082\n",
      "2022-02-26 19:43:43,262 BAD EPOCHS (no improvement): 1\n",
      "2022-02-26 19:43:43,262 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:43:48,198 epoch 60 - iter 46/469 - loss 0.09309727 - samples/sec: 298.76 - lr: 0.025000\n",
      "2022-02-26 19:43:53,213 epoch 60 - iter 92/469 - loss 0.08913968 - samples/sec: 293.65 - lr: 0.025000\n",
      "2022-02-26 19:43:58,182 epoch 60 - iter 138/469 - loss 0.08667808 - samples/sec: 296.39 - lr: 0.025000\n",
      "2022-02-26 19:44:03,214 epoch 60 - iter 184/469 - loss 0.08807833 - samples/sec: 292.91 - lr: 0.025000\n",
      "2022-02-26 19:44:08,317 epoch 60 - iter 230/469 - loss 0.08711605 - samples/sec: 288.61 - lr: 0.025000\n",
      "2022-02-26 19:44:13,449 epoch 60 - iter 276/469 - loss 0.08834649 - samples/sec: 286.97 - lr: 0.025000\n",
      "2022-02-26 19:44:18,626 epoch 60 - iter 322/469 - loss 0.08802910 - samples/sec: 284.61 - lr: 0.025000\n",
      "2022-02-26 19:44:23,799 epoch 60 - iter 368/469 - loss 0.08770011 - samples/sec: 284.87 - lr: 0.025000\n",
      "2022-02-26 19:44:28,946 epoch 60 - iter 414/469 - loss 0.08798386 - samples/sec: 286.11 - lr: 0.025000\n",
      "2022-02-26 19:44:34,025 epoch 60 - iter 460/469 - loss 0.08807218 - samples/sec: 290.03 - lr: 0.025000\n",
      "2022-02-26 19:44:34,936 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:44:34,937 EPOCH 60 done: loss 0.0881 - lr 0.0250000\n",
      "2022-02-26 19:44:42,543 DEV : loss 0.055440038442611694 - f1-score (micro avg)  0.9112\n",
      "2022-02-26 19:44:42,634 BAD EPOCHS (no improvement): 0\n",
      "2022-02-26 19:44:42,635 saving best model\n",
      "2022-02-26 19:44:43,211 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:44:48,353 epoch 61 - iter 46/469 - loss 0.08612124 - samples/sec: 286.58 - lr: 0.025000\n",
      "2022-02-26 19:44:53,489 epoch 61 - iter 92/469 - loss 0.08357400 - samples/sec: 286.79 - lr: 0.025000\n",
      "2022-02-26 19:44:58,581 epoch 61 - iter 138/469 - loss 0.08469520 - samples/sec: 289.27 - lr: 0.025000\n",
      "2022-02-26 19:45:03,643 epoch 61 - iter 184/469 - loss 0.08549039 - samples/sec: 291.00 - lr: 0.025000\n",
      "2022-02-26 19:45:08,858 epoch 61 - iter 230/469 - loss 0.08589112 - samples/sec: 282.53 - lr: 0.025000\n",
      "2022-02-26 19:45:13,992 epoch 61 - iter 276/469 - loss 0.08566891 - samples/sec: 287.04 - lr: 0.025000\n",
      "2022-02-26 19:45:19,082 epoch 61 - iter 322/469 - loss 0.08627547 - samples/sec: 289.40 - lr: 0.025000\n",
      "2022-02-26 19:45:23,962 epoch 61 - iter 368/469 - loss 0.08666756 - samples/sec: 301.75 - lr: 0.025000\n",
      "2022-02-26 19:45:29,073 epoch 61 - iter 414/469 - loss 0.08718314 - samples/sec: 288.30 - lr: 0.025000\n",
      "2022-02-26 19:45:34,251 epoch 61 - iter 460/469 - loss 0.08776166 - samples/sec: 284.42 - lr: 0.025000\n",
      "2022-02-26 19:45:35,188 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:45:35,189 EPOCH 61 done: loss 0.0878 - lr 0.0250000\n",
      "2022-02-26 19:45:43,879 DEV : loss 0.056686870753765106 - f1-score (micro avg)  0.9103\n",
      "2022-02-26 19:45:43,973 BAD EPOCHS (no improvement): 1\n",
      "2022-02-26 19:45:43,974 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:45:49,166 epoch 62 - iter 46/469 - loss 0.09492765 - samples/sec: 283.69 - lr: 0.025000\n",
      "2022-02-26 19:45:54,064 epoch 62 - iter 92/469 - loss 0.09405744 - samples/sec: 300.81 - lr: 0.025000\n",
      "2022-02-26 19:45:58,275 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:45:58,276 Exiting from training early.\n",
      "2022-02-26 19:45:58,276 Saving model ...\n",
      "2022-02-26 19:45:58,826 Done.\n",
      "2022-02-26 19:45:58,827 ----------------------------------------------------------------------------------------------------\n",
      "2022-02-26 19:45:58,828 loading file resources\\taggers\\example-ner\\best-model.pt\n",
      "2022-02-26 19:46:05,699 0.8979\t0.862\t0.8796\t0.8067\n",
      "2022-02-26 19:46:05,700 \n",
      "Results:\n",
      "- F-score (micro) 0.8796\n",
      "- F-score (macro) 0.8621\n",
      "- Accuracy 0.8067\n",
      "\n",
      "By class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC     0.8800    0.9313    0.9050      1646\n",
      "         ORG     0.8863    0.7907    0.8357      1715\n",
      "         PER     0.9661    0.9326    0.9491      1618\n",
      "        MISC     0.8078    0.7151    0.7586       723\n",
      "\n",
      "   micro avg     0.8979    0.8620    0.8796      5702\n",
      "   macro avg     0.8850    0.8424    0.8621      5702\n",
      "weighted avg     0.8972    0.8620    0.8781      5702\n",
      " samples avg     0.8067    0.8067    0.8067      5702\n",
      "\n",
      "2022-02-26 19:46:05,700 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_score': 0.879563350035791,\n",
       " 'dev_score_history': [0.6909984871406959,\n",
       "  0.7693317086887244,\n",
       "  0.8213519313304721,\n",
       "  0.8342826628693485,\n",
       "  0.8522300776817665,\n",
       "  0.859655293703834,\n",
       "  0.8629459248329139,\n",
       "  0.868693909558247,\n",
       "  0.8715236307797303,\n",
       "  0.8771080169506184,\n",
       "  0.8813969571230983,\n",
       "  0.8859550073829584,\n",
       "  0.8830358690001733,\n",
       "  0.8853305785123967,\n",
       "  0.8800480150904569,\n",
       "  0.8919222462203023,\n",
       "  0.8895584057846259,\n",
       "  0.8843806412791198,\n",
       "  0.8913902732428253,\n",
       "  0.8886790833404857,\n",
       "  0.8959890062698617,\n",
       "  0.8968548039638088,\n",
       "  0.9001114636028467,\n",
       "  0.8966108966108967,\n",
       "  0.8994518670777664,\n",
       "  0.89909521757863,\n",
       "  0.9013310433662517,\n",
       "  0.9005988023952095,\n",
       "  0.9016844276383638,\n",
       "  0.9005397070161913,\n",
       "  0.9023887265853239,\n",
       "  0.9040460441542822,\n",
       "  0.9046844223687591,\n",
       "  0.9027064063035287,\n",
       "  0.9005486968449932,\n",
       "  0.9036897525896755,\n",
       "  0.9058288265087132,\n",
       "  0.9047741053890169,\n",
       "  0.9046475733150403,\n",
       "  0.9064166523309823,\n",
       "  0.9049664890874721,\n",
       "  0.9056120269923977,\n",
       "  0.9056150878696958,\n",
       "  0.9060137457044672,\n",
       "  0.9061643835616439,\n",
       "  0.9081702711980777,\n",
       "  0.9085782638414217,\n",
       "  0.9080449901262128,\n",
       "  0.9081860305474515,\n",
       "  0.9075471698113209,\n",
       "  0.9086703221384509,\n",
       "  0.9091685892506195,\n",
       "  0.9074407055398579,\n",
       "  0.9090131691465708,\n",
       "  0.9093715167624109,\n",
       "  0.9074534694227635,\n",
       "  0.9086550809006079,\n",
       "  0.9108809177296464,\n",
       "  0.9081694012541878,\n",
       "  0.9111909650924025,\n",
       "  0.9103058339758416],\n",
       " 'train_loss_history': [0.39049203322173565,\n",
       "  0.2389129387450507,\n",
       "  0.2020616050122408,\n",
       "  0.182394940093681,\n",
       "  0.17130456707836503,\n",
       "  0.16219156912304858,\n",
       "  0.1552077684731271,\n",
       "  0.1490048760795361,\n",
       "  0.14608461391687647,\n",
       "  0.14233903410724147,\n",
       "  0.13871991597714575,\n",
       "  0.13509653980150496,\n",
       "  0.1334311799508376,\n",
       "  0.1302518254918647,\n",
       "  0.12916618660672544,\n",
       "  0.12614495351906876,\n",
       "  0.1243921946878327,\n",
       "  0.12233220862919915,\n",
       "  0.12388118478969082,\n",
       "  0.1223543423468396,\n",
       "  0.1142644195921389,\n",
       "  0.11214563983380425,\n",
       "  0.11168651406641339,\n",
       "  0.10667589026664734,\n",
       "  0.10698112366918792,\n",
       "  0.1072579430343393,\n",
       "  0.10630139443183079,\n",
       "  0.10437369752555051,\n",
       "  0.10441643969823053,\n",
       "  0.1040082642610252,\n",
       "  0.10342440946206091,\n",
       "  0.1022235235109909,\n",
       "  0.1010209301515721,\n",
       "  0.0996571815526638,\n",
       "  0.10086227390922749,\n",
       "  0.10061165539477936,\n",
       "  0.10006809470045935,\n",
       "  0.09998593494506079,\n",
       "  0.09845875391885343,\n",
       "  0.09923469995644305,\n",
       "  0.09724161419982243,\n",
       "  0.09832818834470708,\n",
       "  0.09544852330428524,\n",
       "  0.09758582640081852,\n",
       "  0.09479068698724645,\n",
       "  0.09278826003155834,\n",
       "  0.09360935145989398,\n",
       "  0.09229381682321303,\n",
       "  0.09036810364407803,\n",
       "  0.09119479185960289,\n",
       "  0.09239690048690753,\n",
       "  0.08986927278191009,\n",
       "  0.09145933735373958,\n",
       "  0.0896354432514332,\n",
       "  0.08928960670112746,\n",
       "  0.09044833365266493,\n",
       "  0.08842186110398244,\n",
       "  0.08944556737660894,\n",
       "  0.08926046755154354,\n",
       "  0.08807084707093152,\n",
       "  0.08781485800511941],\n",
       " 'dev_loss_history': [tensor(0.2009, device='cuda:0'),\n",
       "  tensor(0.1504, device='cuda:0'),\n",
       "  tensor(0.1186, device='cuda:0'),\n",
       "  tensor(0.1035, device='cuda:0'),\n",
       "  tensor(0.0900, device='cuda:0'),\n",
       "  tensor(0.0906, device='cuda:0'),\n",
       "  tensor(0.0853, device='cuda:0'),\n",
       "  tensor(0.0850, device='cuda:0'),\n",
       "  tensor(0.0786, device='cuda:0'),\n",
       "  tensor(0.0786, device='cuda:0'),\n",
       "  tensor(0.0748, device='cuda:0'),\n",
       "  tensor(0.0754, device='cuda:0'),\n",
       "  tensor(0.0738, device='cuda:0'),\n",
       "  tensor(0.0706, device='cuda:0'),\n",
       "  tensor(0.0724, device='cuda:0'),\n",
       "  tensor(0.0690, device='cuda:0'),\n",
       "  tensor(0.0681, device='cuda:0'),\n",
       "  tensor(0.0686, device='cuda:0'),\n",
       "  tensor(0.0683, device='cuda:0'),\n",
       "  tensor(0.0666, device='cuda:0'),\n",
       "  tensor(0.0624, device='cuda:0'),\n",
       "  tensor(0.0630, device='cuda:0'),\n",
       "  tensor(0.0602, device='cuda:0'),\n",
       "  tensor(0.0623, device='cuda:0'),\n",
       "  tensor(0.0610, device='cuda:0'),\n",
       "  tensor(0.0610, device='cuda:0'),\n",
       "  tensor(0.0602, device='cuda:0'),\n",
       "  tensor(0.0612, device='cuda:0'),\n",
       "  tensor(0.0631, device='cuda:0'),\n",
       "  tensor(0.0605, device='cuda:0'),\n",
       "  tensor(0.0593, device='cuda:0'),\n",
       "  tensor(0.0582, device='cuda:0'),\n",
       "  tensor(0.0586, device='cuda:0'),\n",
       "  tensor(0.0583, device='cuda:0'),\n",
       "  tensor(0.0582, device='cuda:0'),\n",
       "  tensor(0.0583, device='cuda:0'),\n",
       "  tensor(0.0591, device='cuda:0'),\n",
       "  tensor(0.0576, device='cuda:0'),\n",
       "  tensor(0.0606, device='cuda:0'),\n",
       "  tensor(0.0607, device='cuda:0'),\n",
       "  tensor(0.0589, device='cuda:0'),\n",
       "  tensor(0.0562, device='cuda:0'),\n",
       "  tensor(0.0563, device='cuda:0'),\n",
       "  tensor(0.0587, device='cuda:0'),\n",
       "  tensor(0.0576, device='cuda:0'),\n",
       "  tensor(0.0572, device='cuda:0'),\n",
       "  tensor(0.0567, device='cuda:0'),\n",
       "  tensor(0.0565, device='cuda:0'),\n",
       "  tensor(0.0563, device='cuda:0'),\n",
       "  tensor(0.0578, device='cuda:0'),\n",
       "  tensor(0.0562, device='cuda:0'),\n",
       "  tensor(0.0561, device='cuda:0'),\n",
       "  tensor(0.0566, device='cuda:0'),\n",
       "  tensor(0.0566, device='cuda:0'),\n",
       "  tensor(0.0552, device='cuda:0'),\n",
       "  tensor(0.0560, device='cuda:0'),\n",
       "  tensor(0.0557, device='cuda:0'),\n",
       "  tensor(0.0556, device='cuda:0'),\n",
       "  tensor(0.0573, device='cuda:0'),\n",
       "  tensor(0.0554, device='cuda:0'),\n",
       "  tensor(0.0567, device='cuda:0')]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flair.trainers import ModelTrainer\n",
    "trainer : ModelTrainer = ModelTrainer(tagger, corpus)\n",
    "trainer.train('resources/taggers/example-ner',\n",
    "              learning_rate=0.1,\n",
    "              mini_batch_size=32,\n",
    "              max_epochs=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e56e28-60b7-46f2-9f69-15901cfd4933",
   "metadata": {},
   "source": [
    "#### Testing Model on Sample Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4656b72a-a2ab-4e25-b85b-c7b661e41f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-26 19:53:31,374 loading file C:/Users/karti/Data Science/VMock_NER/CrossWeigh/resources/taggers/example-ner/final-model.pt\n",
      "I love Berlin <B-LOC>\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "# load the trained model\n",
    "model = SequenceTagger.load('C:/Users/karti/Data Science/VMock_NER/CrossWeigh/resources/taggers/example-ner/final-model.pt')\n",
    "# create example sentence\n",
    "sentence = Sentence('I love Berlin')\n",
    "# predict the tags\n",
    "model.predict(sentence)\n",
    "print(sentence.to_tagged_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a850336-51c3-46a3-9c1b-9c32dd2e69c6",
   "metadata": {},
   "source": [
    "## Model Results on Connllpp_test.txt\n",
    "\n",
    "Results:\n",
    "- F-score (micro) 0.8796\n",
    "- F-score (macro) 0.8621\n",
    "- Accuracy 0.8067"
   ]
  },
  {
   "cell_type": "raw",
   "id": "265e0ade-119a-489e-a3ca-eacdff6735f9",
   "metadata": {},
   "source": [
    "By class:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "         LOC     0.8800    0.9313    0.9050      1646\n",
    "         ORG     0.8863    0.7907    0.8357      1715\n",
    "         PER     0.9661    0.9326    0.9491      1618\n",
    "        MISC     0.8078    0.7151    0.7586       723\n",
    "\n",
    "   micro avg     0.8979    0.8620    0.8796      5702\n",
    "   macro avg     0.8850    0.8424    0.8621      5702\n",
    "weighted avg     0.8972    0.8620    0.8781      5702\n",
    " samples avg     0.8067    0.8067    0.8067      5702"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45369289-0c0f-498b-a8bd-217233111e5a",
   "metadata": {},
   "source": [
    "#### Function to Convert Output TSV to Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7f7b9b0b-8bd2-41f8-b750-86833f76f00a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tokens</th>\n",
       "      <th>ner_tags</th>\n",
       "      <th>pred_ner_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[SOCCER, -, JAPAN, GET, LUCKY, WIN, ,, CHINA, ...</td>\n",
       "      <td>[O, O, B-LOC, O, O, O, O, B-LOC, O, O, O, O]</td>\n",
       "      <td>[O, O, B-LOC, O, O, O, O, B-LOC, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[Nadim, Ladki]</td>\n",
       "      <td>[B-PER, I-PER]</td>\n",
       "      <td>[B-PER, I-PER]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[AL-AIN, ,, United, Arab, Emirates, 1996-12-06]</td>\n",
       "      <td>[B-LOC, O, B-LOC, I-LOC, I-LOC, O]</td>\n",
       "      <td>[B-LOC, O, B-LOC, I-LOC, I-LOC, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[Japan, began, the, defence, of, their, Asian,...</td>\n",
       "      <td>[B-LOC, O, O, O, O, O, B-MISC, I-MISC, O, O, O...</td>\n",
       "      <td>[B-LOC, O, O, O, O, O, B-MISC, I-MISC, O, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[But, China, saw, their, luck, desert, them, i...</td>\n",
       "      <td>[O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O,...</td>\n",
       "      <td>[O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                             tokens  \\\n",
       "0  0  [SOCCER, -, JAPAN, GET, LUCKY, WIN, ,, CHINA, ...   \n",
       "1  1                                     [Nadim, Ladki]   \n",
       "2  2    [AL-AIN, ,, United, Arab, Emirates, 1996-12-06]   \n",
       "3  3  [Japan, began, the, defence, of, their, Asian,...   \n",
       "4  4  [But, China, saw, their, luck, desert, them, i...   \n",
       "\n",
       "                                            ner_tags  \\\n",
       "0       [O, O, B-LOC, O, O, O, O, B-LOC, O, O, O, O]   \n",
       "1                                     [B-PER, I-PER]   \n",
       "2                 [B-LOC, O, B-LOC, I-LOC, I-LOC, O]   \n",
       "3  [B-LOC, O, O, O, O, O, B-MISC, I-MISC, O, O, O...   \n",
       "4  [O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O,...   \n",
       "\n",
       "                                       pred_ner_tags  \n",
       "0       [O, O, B-LOC, O, O, O, O, B-LOC, O, O, O, O]  \n",
       "1                                     [B-PER, I-PER]  \n",
       "2                 [B-LOC, O, B-LOC, I-LOC, I-LOC, O]  \n",
       "3  [B-LOC, O, O, O, O, O, B-MISC, I-MISC, O, O, O...  \n",
       "4  [O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O,...  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "# function to get json from tab separated data\n",
    "def generate_examples(filepath):\n",
    "    logging.info(\"⏳ Generating examples from = %s\", filepath)\n",
    "    with open(filepath, encoding=\"utf-8\") as f:\n",
    "        guid = 0\n",
    "        tokens = []\n",
    "        ner_tags = []\n",
    "        pred_ner_tags = []\n",
    "        for line in f:\n",
    "            if line.startswith(\"-DOCSTART-\") or line == \"\" or line == \"\\n\":\n",
    "                if tokens:\n",
    "                    yield {\n",
    "                        \"id\": str(guid),\n",
    "                        \"tokens\": tokens,\n",
    "                        \"ner_tags\": ner_tags,\n",
    "                        \"pred_ner_tags\": pred_ner_tags,\n",
    "                    }\n",
    "                    guid += 1\n",
    "                    tokens = []\n",
    "                    ner_tags = []\n",
    "                    pred_ner_tags = []\n",
    "            else:\n",
    "                # conll2003 tokens are space separated\n",
    "                splits = line.split(\" \")\n",
    "                tokens.append(splits[0])\n",
    "                ner_tags.append(splits[1])\n",
    "                # indexing the output to remove '\\n' from end\n",
    "                pred_ner_tags.append(splits[2][:-1])\n",
    "        # last example\n",
    "        if tokens:\n",
    "            yield {\n",
    "                \"id\": str(guid),\n",
    "                \"tokens\": tokens,\n",
    "                \"ner_tags\": pos_tags,\n",
    "                \"pred_ner_tags\": chunk_tags,\n",
    "            }\n",
    "            \n",
    "test_data = generate_examples('resources/taggers/example-ner/test.tsv')\n",
    "test_df   = pd.DataFrame(test_data)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0888fb29-0e71-412c-bde7-b3d035b1a531",
   "metadata": {},
   "source": [
    "#### Function to Create Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1b7bb940-c68b-4db0-9d91-c5b0c4ed8b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def evaluate(pred, true):\n",
    "    # pred -> list of predicted tags\n",
    "    # true -> lisst of true tags\n",
    "    assert len(pred) == len(true)\n",
    "    return sum(np.array(pred) == np.array(true)) / len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "906c727c-2a80-4aa3-ba1f-3e694589a77e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tokens</th>\n",
       "      <th>ner_tags</th>\n",
       "      <th>pred_ner_tags</th>\n",
       "      <th>evalscores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[SOCCER, -, JAPAN, GET, LUCKY, WIN, ,, CHINA, ...</td>\n",
       "      <td>[O, O, B-LOC, O, O, O, O, B-LOC, O, O, O, O]</td>\n",
       "      <td>[O, O, B-LOC, O, O, O, O, B-LOC, O, O, O, O]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[Nadim, Ladki]</td>\n",
       "      <td>[B-PER, I-PER]</td>\n",
       "      <td>[B-PER, I-PER]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[AL-AIN, ,, United, Arab, Emirates, 1996-12-06]</td>\n",
       "      <td>[B-LOC, O, B-LOC, I-LOC, I-LOC, O]</td>\n",
       "      <td>[B-LOC, O, B-LOC, I-LOC, I-LOC, O]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[Japan, began, the, defence, of, their, Asian,...</td>\n",
       "      <td>[B-LOC, O, O, O, O, O, B-MISC, I-MISC, O, O, O...</td>\n",
       "      <td>[B-LOC, O, O, O, O, O, B-MISC, I-MISC, O, O, O...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[But, China, saw, their, luck, desert, them, i...</td>\n",
       "      <td>[O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O,...</td>\n",
       "      <td>[O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O,...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                             tokens  \\\n",
       "0  0  [SOCCER, -, JAPAN, GET, LUCKY, WIN, ,, CHINA, ...   \n",
       "1  1                                     [Nadim, Ladki]   \n",
       "2  2    [AL-AIN, ,, United, Arab, Emirates, 1996-12-06]   \n",
       "3  3  [Japan, began, the, defence, of, their, Asian,...   \n",
       "4  4  [But, China, saw, their, luck, desert, them, i...   \n",
       "\n",
       "                                            ner_tags  \\\n",
       "0       [O, O, B-LOC, O, O, O, O, B-LOC, O, O, O, O]   \n",
       "1                                     [B-PER, I-PER]   \n",
       "2                 [B-LOC, O, B-LOC, I-LOC, I-LOC, O]   \n",
       "3  [B-LOC, O, O, O, O, O, B-MISC, I-MISC, O, O, O...   \n",
       "4  [O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O,...   \n",
       "\n",
       "                                       pred_ner_tags  evalscores  \n",
       "0       [O, O, B-LOC, O, O, O, O, B-LOC, O, O, O, O]         1.0  \n",
       "1                                     [B-PER, I-PER]         1.0  \n",
       "2                 [B-LOC, O, B-LOC, I-LOC, I-LOC, O]         1.0  \n",
       "3  [B-LOC, O, O, O, O, O, B-MISC, I-MISC, O, O, O...         1.0  \n",
       "4  [O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O,...         1.0  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['evalscores'] = test_df.apply(lambda x: evaluate(x['pred_ner_tags'], x['ner_tags']), axis=1)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cfcb22-162b-4eef-aaf0-7e89c9c0baa7",
   "metadata": {},
   "source": [
    "#### 5 of the Perfectly predicted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bc5d376c-3a61-41c3-b895-1dc4c0169017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tokens</th>\n",
       "      <th>ner_tags</th>\n",
       "      <th>pred_ner_tags</th>\n",
       "      <th>evalscores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[SOCCER, -, JAPAN, GET, LUCKY, WIN, ,, CHINA, IN, SURPRISE, DEFEAT, .]</td>\n",
       "      <td>[O, O, B-LOC, O, O, O, O, B-LOC, O, O, O, O]</td>\n",
       "      <td>[O, O, B-LOC, O, O, O, O, B-LOC, O, O, O, O]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[Nadim, Ladki]</td>\n",
       "      <td>[B-PER, I-PER]</td>\n",
       "      <td>[B-PER, I-PER]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[AL-AIN, ,, United, Arab, Emirates, 1996-12-06]</td>\n",
       "      <td>[B-LOC, O, B-LOC, I-LOC, I-LOC, O]</td>\n",
       "      <td>[B-LOC, O, B-LOC, I-LOC, I-LOC, O]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[Japan, began, the, defence, of, their, Asian, Cup, title, with, a, lucky, 2-1, win, against, Syria, in, a, Group, C, championship, match, on, Friday, .]</td>\n",
       "      <td>[B-LOC, O, O, O, O, O, B-MISC, I-MISC, O, O, O, O, O, O, O, B-LOC, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[B-LOC, O, O, O, O, O, B-MISC, I-MISC, O, O, O, O, O, O, O, B-LOC, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[But, China, saw, their, luck, desert, them, in, the, second, match, of, the, group, ,, crashing, to, a, surprise, 2-0, defeat, to, newcomers, Uzbekistan, .]</td>\n",
       "      <td>[O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-LOC, O]</td>\n",
       "      <td>[O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-LOC, O]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id  \\\n",
       "0  0   \n",
       "1  1   \n",
       "2  2   \n",
       "3  3   \n",
       "4  4   \n",
       "\n",
       "                                                                                                                                                          tokens  \\\n",
       "0                                                                                         [SOCCER, -, JAPAN, GET, LUCKY, WIN, ,, CHINA, IN, SURPRISE, DEFEAT, .]   \n",
       "1                                                                                                                                                 [Nadim, Ladki]   \n",
       "2                                                                                                                [AL-AIN, ,, United, Arab, Emirates, 1996-12-06]   \n",
       "3      [Japan, began, the, defence, of, their, Asian, Cup, title, with, a, lucky, 2-1, win, against, Syria, in, a, Group, C, championship, match, on, Friday, .]   \n",
       "4  [But, China, saw, their, luck, desert, them, in, the, second, match, of, the, group, ,, crashing, to, a, surprise, 2-0, defeat, to, newcomers, Uzbekistan, .]   \n",
       "\n",
       "                                                                                        ner_tags  \\\n",
       "0                                                   [O, O, B-LOC, O, O, O, O, B-LOC, O, O, O, O]   \n",
       "1                                                                                 [B-PER, I-PER]   \n",
       "2                                                             [B-LOC, O, B-LOC, I-LOC, I-LOC, O]   \n",
       "3  [B-LOC, O, O, O, O, O, B-MISC, I-MISC, O, O, O, O, O, O, O, B-LOC, O, O, O, O, O, O, O, O, O]   \n",
       "4            [O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-LOC, O]   \n",
       "\n",
       "                                                                                   pred_ner_tags  \\\n",
       "0                                                   [O, O, B-LOC, O, O, O, O, B-LOC, O, O, O, O]   \n",
       "1                                                                                 [B-PER, I-PER]   \n",
       "2                                                             [B-LOC, O, B-LOC, I-LOC, I-LOC, O]   \n",
       "3  [B-LOC, O, O, O, O, O, B-MISC, I-MISC, O, O, O, O, O, O, O, B-LOC, O, O, O, O, O, O, O, O, O]   \n",
       "4            [O, B-LOC, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-LOC, O]   \n",
       "\n",
       "   evalscores  \n",
       "0         1.0  \n",
       "1         1.0  \n",
       "2         1.0  \n",
       "3         1.0  \n",
       "4         1.0  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "test_df[test_df['evalscores'] == 1].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17591c4-103a-4be8-8cb1-3df9cdabbc30",
   "metadata": {},
   "source": [
    "#### 5 of the Poorly predicted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e9230709-3f8e-4941-b7a0-c44842c8f08c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tokens</th>\n",
       "      <th>ner_tags</th>\n",
       "      <th>pred_ner_tags</th>\n",
       "      <th>evalscores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>225</td>\n",
       "      <td>[REUTER]</td>\n",
       "      <td>[B-ORG]</td>\n",
       "      <td>[B-PER]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>359</td>\n",
       "      <td>[NORTHEAST, DIVISION]</td>\n",
       "      <td>[O, O]</td>\n",
       "      <td>[B-MISC, I-MISC]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>385</td>\n",
       "      <td>[PACIFIC, DIVISION]</td>\n",
       "      <td>[B-LOC, O]</td>\n",
       "      <td>[B-MISC, I-MISC]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>457</td>\n",
       "      <td>[MIDWEST, DIVISION]</td>\n",
       "      <td>[O, O]</td>\n",
       "      <td>[B-MISC, I-MISC]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>466</td>\n",
       "      <td>[PACIFIC, DIVISION]</td>\n",
       "      <td>[B-LOC, O]</td>\n",
       "      <td>[B-MISC, I-MISC]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                 tokens    ner_tags     pred_ner_tags  evalscores\n",
       "225  225               [REUTER]     [B-ORG]           [B-PER]         0.0\n",
       "359  359  [NORTHEAST, DIVISION]      [O, O]  [B-MISC, I-MISC]         0.0\n",
       "385  385    [PACIFIC, DIVISION]  [B-LOC, O]  [B-MISC, I-MISC]         0.0\n",
       "457  457    [MIDWEST, DIVISION]      [O, O]  [B-MISC, I-MISC]         0.0\n",
       "466  466    [PACIFIC, DIVISION]  [B-LOC, O]  [B-MISC, I-MISC]         0.0"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[test_df['evalscores'] == 0].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de2afb9-85a3-4170-b95b-d0141b3453bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
